{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import torch \n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def geometricPad(grid, pad=1):\n",
    "  M = grid.shape[2]; # 0:batchsize, 1:channels, 2:width, 3:height\n",
    "  M_new = pad+M+pad;\n",
    "  new_shape = (grid.shape[0], grid.shape[1], M_new, M_new)\n",
    "  grid_new = grid.new_zeros(new_shape);\n",
    "  grid_new[:,:,pad:M+pad,pad:M+pad] = grid;\n",
    "  grid_new[:,:,0:pad,pad:(M+pad)] = grid[:,:,:,0:pad].flip(-1).transpose(-1,-2);\n",
    "  grid_new[:,:,pad:(M+pad),0:pad] = grid[:,:,0:pad,:].flip(-2).transpose(-1,-2);\n",
    "  grid_new[:,:,(M+pad):(M+pad+pad),(pad):(M+pad)] = grid[:,:,:,(M-pad):].flip(-1).transpose(-1,-2);\n",
    "  grid_new[:,:,pad:(M+pad),(M+pad):(M+pad+pad)] = grid[:,:,(M-pad):,:].flip(-2).transpose(-1,-2);\n",
    "  return(grid_new);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshowRdBu(ax,img):\n",
    "    minmax = max(torch.max(img),-torch.min(img))\n",
    "    return ax.imshow(-img,cmap='RdBu',vmin=-minmax,vmax=minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class H5Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, files, transform=None, flavour=None, limit_num_files=0, start_fraction=0., use_fraction=1.0):\n",
    "        \"\"\"                                                                                                                                             \n",
    "        Args: data_dirs ... a list of data directories to find files (up to 10 files read from each dir)                                                \n",
    "              transform ... a function applied to pre-process data                                                                                      \n",
    "              flavour ..... a string that is required to be present in the filename                                                                     \n",
    "              limit_num_files ... an integer limiting number of files to be taken per data directory                                                    \n",
    "              start_fraction ... a floating point fraction (0.0=>1.0) to specify which entry to start reading (per file)                                \n",
    "              use_fraction ..... a floating point fraction (0.0=>1.0) to specify how much fraction of a file to be read out (per file)                  \n",
    "        \"\"\"\n",
    "        self._transform = transform\n",
    "        self._files = []\n",
    "\n",
    "        # Check input fractions makes sense                                                                                                             \n",
    "        assert start_fraction >= 0. and start_fraction < 1.\n",
    "        assert use_fraction > 0. and use_fraction <= 1.\n",
    "        assert (start_fraction + use_fraction) <= 1.\n",
    "\n",
    "        # Load files (up to 10) from each directory in data_dirs list                                                                                   \n",
    "        # for d in data_dirs:\n",
    "        #     file_list = [ os.path.join(d,f) for f in os.listdir(d) if flavour is None or flavour in f ]\n",
    "        #     if limit_num_files: file_list = file_list[0:limit_num_files]\n",
    "        #     self._files += file_list\n",
    "        self._files = files\n",
    "\n",
    "        self._file_handles = [None] * len(self._files)\n",
    "        self._event_to_file_index  = []\n",
    "        self._event_to_entry_index = []\n",
    "        import h5py\n",
    "        for file_index, file_name in enumerate(self._files):\n",
    "            f = h5py.File(file_name,mode='r')\n",
    "            data_size = f['event_data'].shape[0]\n",
    "            start_entry = int(start_fraction * data_size)\n",
    "            num_entries = int(use_fraction * data_size)\n",
    "            self._event_to_file_index += [file_index] * num_entries\n",
    "            self._event_to_entry_index += range(start_entry, start_entry+num_entries)\n",
    "            f.close()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._event_to_file_index)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        file_index = self._event_to_file_index[idx]\n",
    "        entry_index = self._event_to_entry_index[idx]\n",
    "        if self._file_handles[file_index] is None:\n",
    "            import h5py\n",
    "            self._file_handles[file_index] = h5py.File(self._files[file_index],mode='r')\n",
    "        fh = self._file_handles[file_index]\n",
    "        label = fh['labels'][entry_index]\n",
    "        #labelTranslation = [-1,0,1,-1,2] # make sure that labels 1,2,4 get values 0,1,2\n",
    "        if label == 1: # electron\n",
    "            label = 0\n",
    "        elif label == 2: # muon\n",
    "            label = 1\n",
    "        elif label == 4: # pip\n",
    "            label = 2\n",
    "        else:\n",
    "            print('Unknown label', label, 'for entry_index', entry_index, 'treating as label=0')\n",
    "            label = 0\n",
    "        \n",
    "        # try:\n",
    "        #     label = labelTranslation[label]\n",
    "        # except IndexError:\n",
    "        #     print('IndexError at entry', entry_index, 'label in file was', fh['labels'][entry_index], 'translations are', labelTranslation)\n",
    "        #     raise\n",
    "        \n",
    "        event_data = fh['event_data'][entry_index]\n",
    "        return event_data[:,:,0:1],label,idx\n",
    "\n",
    "def HKCollate(batch):\n",
    "    data  = np.stack([sample[0] for sample in batch])\n",
    "    label = [sample[1] for sample in batch]\n",
    "    idx   = [sample[2] for sample in batch]\n",
    "    return data,label,idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also add more data\n",
    "\n",
    "#mcdir = '/home/lukasb/projects/rpp-tanaka-ab/lukasb/watchmal/20190808-uniisoflat'\n",
    "mcdir = '/home/lukasb/scratch/20190808-uniisoflat'\n",
    "wallmat = 'blacksheet'\n",
    "pnames = ('e','mu','pip')\n",
    "files = ['%s/1R%s-%s/wc-0-bch%d.h5' % (mcdir,pname,wallmat,bch+1) for bch in range(100) for pname in pnames]\n",
    "\n",
    "train_ds = H5Dataset(files,start_fraction=0.0,use_fraction=0.9);\n",
    "test_ds  = H5Dataset(files,start_fraction=0.9,use_fraction=0.1);\n",
    "\n",
    "# for training\n",
    "train_loader=DataLoader(train_ds,batch_size= 64,shuffle=True,num_workers=4,collate_fn=HKCollate)\n",
    "# for validation\n",
    "test_loader =DataLoader( test_ds,batch_size=200,shuffle=True,num_workers=2,collate_fn=HKCollate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BNConv2d(torch.nn.Module):\n",
    "    def __init__(self,Cin,Cout,pad):\n",
    "        super(BNConv2d, self).__init__()\n",
    "        self.batchNorm2d = torch.nn.BatchNorm2d(Cin)\n",
    "        self.conv2d = torch.nn.Conv2d(Cin,Cout,2*pad+1,padding=pad)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.batchNorm2d(x)\n",
    "        return self.conv2d(x)\n",
    "\n",
    "class GeomBNConv2d(torch.nn.Module):\n",
    "    def __init__(self,Cin,Cout,pad):\n",
    "        super(GeomBNConv2d, self).__init__()\n",
    "        self.Cin = Cin\n",
    "        self.Cout = Cout\n",
    "        self.pad = pad\n",
    "        self.batchNorm2d = torch.nn.BatchNorm2d(Cin)\n",
    "        self.conv2d = torch.nn.Conv2d(Cin,Cout,2*self.pad+1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.batchNorm2d(x)\n",
    "        inputsPad = geometricPad(x, self.pad); # (B,C,W,H)\n",
    "        return self.conv2d(inputsPad)\n",
    "\n",
    "class TaylorBNConv2d(torch.nn.Module):\n",
    "    def __init__(self,Cin,Cout,taylor):\n",
    "        super(TaylorBNConv2d, self).__init__()\n",
    "        self.Cin = Cin\n",
    "        self.Cout = Cout\n",
    "        self.taylor = taylor\n",
    "        self.batchNorm2d = torch.nn.BatchNorm2d(Cin*taylor.Ntaylor)\n",
    "        self.conv2d = torch.nn.Conv2d(Cin*taylor.Ntaylor,Cout,1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.taylor(x)\n",
    "        x = self.batchNorm2d(x)\n",
    "        x = self.conv2d(x)\n",
    "        return x\n",
    "    \n",
    "    # the default implementation of cpu() and cuda() only passes this along to nn.Module instances\n",
    "    # since self.AA is a tensor, we need to take care of it ourselves\n",
    "    \n",
    "    def cpu(self):\n",
    "        super(TaylorBNConv2d, self).cpu()\n",
    "        self.taylor.cpu()\n",
    "        return self\n",
    "    \n",
    "    def cuda(self):\n",
    "        super(TaylorBNConv2d, self).cuda()\n",
    "        self.taylor.cuda()\n",
    "        return self\n",
    "\n",
    "\n",
    "class BNCNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_class):\n",
    "        super(BNCNN, self).__init__()\n",
    "        \n",
    "        # feature extractor CNN\n",
    "        self._feature = torch.nn.Sequential(\n",
    "            BNConv2d( 1,16,1), torch.nn.ReLU(),\n",
    "            BNConv2d(16,32,1), torch.nn.ReLU(),\n",
    "            \n",
    "            torch.nn.MaxPool2d(2,2),\n",
    "            BNConv2d(32,32,1), torch.nn.ReLU(),\n",
    "            BNConv2d(32,32,1), torch.nn.ReLU(),\n",
    "            \n",
    "            torch.nn.MaxPool2d(2,2),\n",
    "            BNConv2d(32,64,1), torch.nn.ReLU(),\n",
    "            BNConv2d(64,64,1), torch.nn.ReLU(),\n",
    "            \n",
    "            torch.nn.MaxPool2d(2,2),\n",
    "            BNConv2d( 64,128,1), torch.nn.ReLU(),\n",
    "            BNConv2d(128,128,1), torch.nn.ReLU()\n",
    "        )\n",
    "        self._classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128,32), torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32,num_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        net = self._feature(x)\n",
    "        net = torch.nn.AvgPool2d(net.size()[2:])(net)\n",
    "        return self._classifier(net.view(-1,128))\n",
    "    \n",
    "    # I'm not sure why I need to implement cpu/cuda myself, train works somehow\n",
    "    \n",
    "    def cpu(self):\n",
    "        super(BNCNN, self).cpu()\n",
    "        for module in self._feature:\n",
    "            module.cpu()\n",
    "        for module in self._classifier:\n",
    "            module.cpu()\n",
    "        return self\n",
    "    \n",
    "    def cuda(self):\n",
    "        super(BNCNN, self).cuda()\n",
    "        for module in self._feature:\n",
    "            module.cuda()\n",
    "        for module in self._classifier:\n",
    "            module.cuda()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(blob,train=True):\n",
    "    \"\"\"\n",
    "       Args: blob should have attributes, net, criterion, softmax, data, label\n",
    "       Returns: a dictionary of predicted labels, softmax, loss, and accuracy\n",
    "    \"\"\"\n",
    "    with torch.set_grad_enabled(train):\n",
    "        # Prediction\n",
    "        data = torch.as_tensor(blob.data).cuda()#[torch.as_tensor(d).cuda() for d in blob.data]\n",
    "        data = data.permute(0,3,1,2)\n",
    "        prediction = blob.net(data)\n",
    "        # Training\n",
    "        loss,acc=-1,-1\n",
    "        if blob.label is not None:\n",
    "            label = torch.as_tensor(blob.label).type(torch.LongTensor).cuda()#[torch.as_tensor(l).cuda() for l in blob.label]\n",
    "            #label = torch.as_tensor(blob.label).type(torch.FloatTensor).cuda()#[torch.as_tensor(l).cuda() for l in blob.label]\n",
    "            label.requires_grad = False\n",
    "            loss = blob.criterion(prediction,label)\n",
    "        blob.loss = loss\n",
    "        \n",
    "        softmax    = blob.softmax(prediction).cpu().detach().numpy()\n",
    "        prediction = torch.argmax(prediction,dim=-1)\n",
    "        accuracy   = (prediction == label).sum().item() / float(prediction.nelement())\n",
    "        # mse        = blob.mse(prediction,label).cpu().detach().numpy()\n",
    "        # accuracy   = np.sqrt(mse.mean()).item()\n",
    "        prediction = prediction.cpu().detach().numpy()\n",
    "        \n",
    "        return {'prediction' : prediction,\n",
    "                'softmax'    : softmax,\n",
    "                'loss'       : loss.cpu().detach().item(),\n",
    "                'accuracy'   : accuracy}\n",
    "\n",
    "def backward(blob):\n",
    "    blob.optimizer.zero_grad()  # Reset gradients accumulation\n",
    "    blob.loss.backward()\n",
    "    blob.optimizer.step()\n",
    "\n",
    "def save_state(blob):\n",
    "    # Output file name\n",
    "    filename = '%s-%d.ckpt' % (blob.prefix, blob.iteration)\n",
    "    # Save parameters\n",
    "    # 0+1) iteration counter + optimizer state => in case we want to \"continue training\" later\n",
    "    # 2) network weight\n",
    "    torch.save({\n",
    "        'global_epoch': blob.epoch,\n",
    "        'global_step': blob.iteration,\n",
    "        'optimizer': blob.optimizer.state_dict(),\n",
    "        'state_dict': blob.net.state_dict()\n",
    "        }, filename)\n",
    "    return filename\n",
    "\n",
    "def restore_state(blob, iteration):\n",
    "    # Open a file in read-binary mode\n",
    "    weight_file = '%s-%d.ckpt' % (prefix.prefix, iteration)\n",
    "    with open(weight_file, 'rb') as f:\n",
    "        # torch interprets the file, then we can access using string keys\n",
    "        checkpoint = torch.load(f)\n",
    "        # load network weights\n",
    "        blob.net.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "        # if optimizer is provided, load the state of the optimizer\n",
    "        if blob.optimizer is not None:\n",
    "            blob.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        # load iteration count\n",
    "        blob.epoch     = checkpoint['global_epoch']\n",
    "        blob.iteration = checkpoint['global_step']\n",
    "\n",
    "# weight_file = save_state(blob, '20190819-02-DeepTaylor-01-BatchNorm')\n",
    "# print('Saved to', weight_file)\n",
    "\n",
    "\n",
    "# # Recreate the network (i.e. initialize)\n",
    "# blob.net=LeNet().cuda()\n",
    "# # Get one batch of data to test\n",
    "# blob.data, blob.label = next(iter(train_loader))\n",
    "# # Run forward function\n",
    "# res = forward(blob,True)\n",
    "# # Report\n",
    "# print('Accuracy:',res['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# # Restore the state\n",
    "# restore_state(weight_file,blob)\n",
    "# # Run the forward function\n",
    "# res = forward(blob,True)\n",
    "# # Report\n",
    "# print('Accuracy',res['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLOB:\n",
    "    pass\n",
    "blob=BLOB()\n",
    "blob.net       = BNCNN(3).cuda() # construct CNN for 1 variable regression, use GPU\n",
    "blob.criterion = torch.nn.CrossEntropyLoss() # use softmax loss to define an error\n",
    "blob.optimizer = torch.optim.Adam(blob.net.parameters()) # use Adam optimizer algorithm\n",
    "blob.softmax   = torch.nn.Softmax(dim=1) # not for training, but softmax score for each class\n",
    "blob.prefix    = '20190819-08-DeepConv-trainMore-50k-01-BNCNN'\n",
    "blob.epoch     = 0.\n",
    "blob.iteration = 0\n",
    "blob.data      = None # data for training/analysis\n",
    "blob.label     = None # label for training/analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Starting @ 2019-08-22 13:39:46\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress \n",
       "            value='2110'\n",
       "            max='2110',\n",
       "            style='width: 30%'\n",
       "        >\n",
       "            2110\n",
       "        </progress> 100% ... Iteration 2110 ... Epoch 1.00 ... Loss 0.472 ... Accuracy 0.833\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Starting @ 2019-08-22 13:45:17\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress \n",
       "            value='240'\n",
       "            max='2110',\n",
       "            style='width: 30%'\n",
       "        >\n",
       "            240\n",
       "        </progress> 11% ... Iteration 2350 ... Epoch 1.11 ... Loss 0.432 ... Accuracy 0.828\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "from utils.utils import progress_bar, CSVData\n",
    "\n",
    "blob.train_log, blob.test_log = CSVData('%s-log_train.csv' % blob.prefix), CSVData('%s-log_test.csv' % blob.prefix)\n",
    "\n",
    "# Define train period. \"epoch\" = N image consumption where N is the total number of train samples.\n",
    "TRAIN_EPOCH=4.0\n",
    "# Set the network to training mode\n",
    "blob.net.train()\n",
    "\n",
    "# Start training\n",
    "while int(blob.epoch+0.5) < TRAIN_EPOCH:\n",
    "    print('Epoch',int(blob.epoch+0.5),'Starting @',time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "    # Create a progress bar for this epoch\n",
    "    progress = display(progress_bar(0,len(train_loader)),display_id=True)\n",
    "    # Loop over data samples and into the network forward function\n",
    "    for i,data in enumerate(train_loader):\n",
    "        # Data and label\n",
    "        blob.data,blob.label = data[0:2]\n",
    "        # Call forward: make a prediction & measure the average error\n",
    "        res = forward(blob,True)\n",
    "        # Call backward: backpropagate error and update weights\n",
    "        backward(blob)\n",
    "        # Epoch update\n",
    "        blob.epoch += 1./len(train_loader)\n",
    "        blob.iteration += 1\n",
    "        \n",
    "        #\n",
    "        # Log/Report\n",
    "        #\n",
    "        # Record the current performance on train set\n",
    "        blob.train_log.record(['iteration','epoch','accuracy','loss'],[blob.iteration,blob.epoch,res['accuracy'],res['loss']])\n",
    "        blob.train_log.write()\n",
    "        # once in a while, report\n",
    "        if i==0 or (i+1)%10 == 0:\n",
    "            message = '... Iteration %d ... Epoch %1.2f ... Loss %1.3f ... Accuracy %1.3f' % (blob.iteration,blob.epoch,res['loss'],res['accuracy'])\n",
    "            progress.update(progress_bar((i+1),len(train_loader),message))\n",
    "        # more rarely, run validation\n",
    "        if (i+1)%40 == 0:\n",
    "            with torch.no_grad():\n",
    "                blob.net.eval()\n",
    "                test_data = next(iter(test_loader))\n",
    "                blob.data,blob.label = test_data[0:2]\n",
    "                res = forward(blob,False)\n",
    "                blob.test_log.record(['iteration','epoch','accuracy','loss'],[blob.iteration,blob.epoch,res['accuracy'],res['loss']])\n",
    "                blob.test_log.write()\n",
    "            blob.net.train()\n",
    "        # even more rarely, save state\n",
    "        if (i+1)%400 == 0:\n",
    "            with torch.no_grad():\n",
    "                blob.net.eval()\n",
    "                save_state(blob)\n",
    "            blob.net.train()\n",
    "        if blob.epoch >= TRAIN_EPOCH:\n",
    "            break\n",
    "    message = '... Iteration %d ... Epoch %1.2f ... Loss %1.3f ... Accuracy %1.3f' % (blob.iteration,blob.epoch,res['loss'],res['accuracy'])\n",
    "    #print(message)\n",
    "    progress.update(progress_bar((i+1),len(train_loader),message))\n",
    "\n",
    "blob.test_log.close()\n",
    "blob.train_log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train_log = pd.read_csv(blob.train_log.name)\n",
    "test_log  = pd.read_csv(blob.test_log.name)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12,8),facecolor='w')\n",
    "line11 = ax1.plot(train_log.epoch, train_log.loss, linewidth=2, label='Train loss', color='b', alpha=0.3)\n",
    "line12 = ax1.plot(test_log.epoch, test_log.loss, marker='o', markersize=12, linestyle='', label='Test loss', color='blue')\n",
    "ax1.set_xlabel('Epoch',fontweight='bold',fontsize=24,color='black')\n",
    "ax1.tick_params('x',colors='black',labelsize=18)\n",
    "ax1.set_ylabel('Loss', fontsize=24, fontweight='bold',color='b')\n",
    "ax1.tick_params('y',colors='b',labelsize=18)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "line21 = ax2.plot(train_log.epoch, train_log.accuracy, linewidth=2, label='Train accuracy', color='r', alpha=0.3)\n",
    "line22 = ax2.plot(test_log.epoch, test_log.accuracy, marker='o', markersize=12, linestyle='', label='Test accuracy', color='red')\n",
    "\n",
    "ax2.set_ylabel('Accuracy', fontsize=24, fontweight='bold',color='r')\n",
    "ax2.tick_params('y',colors='r',labelsize=18)\n",
    "ax2.set_ylim(0.,1.0)\n",
    "\n",
    "# added these four lines\n",
    "lines  = line11 + line12 + line21 + line22\n",
    "labels = [l.get_label() for l in lines]\n",
    "leg    = ax1.legend(lines, labels, fontsize=16, loc=5)\n",
    "leg_frame = leg.get_frame()\n",
    "leg_frame.set_facecolor('white')\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(blob,data_loader):\n",
    "    label,prediction,accuracy=[],[],[]\n",
    "    with torch.no_grad():\n",
    "        # set the network to test (non-train) mode\n",
    "        blob.net.eval()\n",
    "        # create the result holder\n",
    "        index,label,prediction = [],[],[]\n",
    "        for i,data in enumerate(data_loader):\n",
    "            blob.data, blob.label = data[0:2]\n",
    "            res = forward(blob,True)\n",
    "            accuracy.append(res['accuracy'])\n",
    "            prediction.append(res['prediction'])\n",
    "            label.append(blob.label)\n",
    "            #if i==2: break\n",
    "        # report accuracy\n",
    "        del blob.data\n",
    "        del blob.label\n",
    "        del data\n",
    "\n",
    "        accuracy   = np.array(accuracy,dtype=np.float32)\n",
    "        label      = np.hstack(label)\n",
    "        prediction = np.hstack(prediction)\n",
    "    \n",
    "    return accuracy, label, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import plot_confusion_matrix\n",
    "\n",
    "inference_loader =DataLoader( test_ds,batch_size=64,shuffle=True,num_workers=2,collate_fn=HKCollate)\n",
    "\n",
    "accuracy,label,prediction = inference(blob,inference_loader)\n",
    "print('Accuracy mean',accuracy.mean(),'std',accuracy.std())\n",
    "plot_confusion_matrix(label,prediction,['e','mu','pip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    Nrows = 5\n",
    "    for r in range(Nrows):\n",
    "        Nplot = 6\n",
    "        fig, axes = plt.subplots(1, Nplot, figsize=(16,3),facecolor='w')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            blob.net.eval()\n",
    "            test_data = next(iter(test_loader))\n",
    "            blob.data,blob.label = test_data[0:2]\n",
    "            blob.data  = blob.data[0:Nplot,:,:,:]\n",
    "            blob.label = blob.label[0:Nplot]\n",
    "            res = forward(blob,True)\n",
    "\n",
    "            for ev in range(Nplot):\n",
    "                im = axes[ev].imshow(blob.data[ev,:,:,0])\n",
    "                labelNames = ['e','μ','π+']\n",
    "                correct = (blob.label[ev] == res['prediction'][ev])\n",
    "                axes[ev].set_title('%s pred: %s, true: %s' % ('✔︎' if correct else '✖︎', labelNames[blob.label[ev]], labelNames[res['prediction'][ev]]))\n",
    "                #cbar = fig.colorbar(im, ax=axes[0])\n",
    "            plt.show()\n",
    "            \n",
    "            del test_data\n",
    "            del blob.data\n",
    "            del blob.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
