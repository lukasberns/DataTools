{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import torch \n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def geometricPad(grid, pad=1):\n",
    "  M = grid.shape[2]; # 0:batchsize, 1:channels, 2:width, 3:height\n",
    "  M_new = pad+M+pad;\n",
    "  new_shape = (grid.shape[0], grid.shape[1], M_new, M_new)\n",
    "  grid_new = grid.new_zeros(new_shape);\n",
    "  grid_new[:,:,pad:M+pad,pad:M+pad] = grid;\n",
    "  grid_new[:,:,0:pad,pad:(M+pad)] = grid[:,:,:,0:pad].flip(-1).transpose(-1,-2);\n",
    "  grid_new[:,:,pad:(M+pad),0:pad] = grid[:,:,0:pad,:].flip(-2).transpose(-1,-2);\n",
    "  grid_new[:,:,(M+pad):(M+pad+pad),(pad):(M+pad)] = grid[:,:,:,(M-pad):].flip(-1).transpose(-1,-2);\n",
    "  grid_new[:,:,pad:(M+pad),(M+pad):(M+pad+pad)] = grid[:,:,(M-pad):,:].flip(-2).transpose(-1,-2);\n",
    "  return(grid_new);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('/home/lukasb/watchmal/data_ssd/IWCDmPMT_4pi_full_tank/h5_topo/e-/IWCDmPMT_4pi_full_tank_e-_E0to1000MeV_unif-pos-R371-y521cm_4pi-dir_3000evts_1.h5','r')\n",
    "event_data = f['event_data'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(event_data[2,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(event_data[:,:,:,1].reshape(-1),bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(event_data[:,:,:,1][event_data[:,:,:,1] > 600.].reshape(-1),bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(np.where(event_data[:,:,:,1] < 600., 2000., event_data[:,:,:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.where(event_data[:,:,:,1] < 600., -2000., event_data[:,:,:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evQ = event_data[:,:,:,0];\n",
    "evT = event_data[:,:,:,1];\n",
    "evR = np.power(evQ,0.25);\n",
    "evA = 2*np.pi*(evT-960.)/(1900.-600.)\n",
    "evX = evR*np.cos(evA);\n",
    "evY = evR*np.sin(evA);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['positions'][0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshowRdBu(ax,img):\n",
    "    minmax = max(torch.max(img),-torch.min(img))\n",
    "    return ax.imshow(-img,cmap='RdBu',vmin=-minmax,vmax=minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshowRdBu(plt,torch.as_tensor(evX[2,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshowRdBu(plt,torch.as_tensor(evY[2,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.stack([evX[2,:,:],evY[2,:,:]],2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taylor prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Taylorizer:\n",
    "    \"\"\"Perform taylor expansion of tensor using pre-computed expansion weights\"\"\"\n",
    "    def __init__(self, mGridPmt, mGridCoord, vectorStartIndices, pad=1, Lambda=0.5):\n",
    "        \"\"\"\n",
    "        mGridPmt:   numpy array with (Height,Width) only significance is\n",
    "                    1. value should be unique for each grid site\n",
    "                    2. if no PMT exists, set to -1\n",
    "        mGridCoord: numpy array with (Ncoords,Height,Width)\n",
    "        vectorStartIndices: array with the start indices of xyz vectors in Ncoords, e.g.\n",
    "                            for [x,y,z,dirx,diry,dirz] pass [0,3] which treats (x,y,z) and (dirx,diry,dirz)\n",
    "                            as a vector\n",
    "        pad:        the taylor expansion will be performed on a grid with sides 2*pad+1\n",
    "        Lambda:     the regularization strength. should be something of order the differences in mGridCoord\n",
    "        \"\"\"\n",
    "        \n",
    "        self.mGridPmt   = mGridPmt\n",
    "        self.mGridCoord = mGridCoord\n",
    "        self.vectorStartIndices = vectorStartIndices\n",
    "        self.pad = pad\n",
    "        self.Lambda = Lambda\n",
    "        self.rotPhi = 0.\n",
    "        self.AA = self.computeTaylorWeights()\n",
    "    \n",
    "    def cuda(self):\n",
    "        self.AA = self.AA.cuda()\n",
    "    \n",
    "    def cpu(self):\n",
    "        self.AA = self.AA.cpu()\n",
    "        \n",
    "    def __call__(self, inputs):\n",
    "        return self.taylorize(inputs)\n",
    "\n",
    "    def computeTaylorWeights(self):\n",
    "        pad = self.pad\n",
    "        mGridPmt = self.mGridPmt\n",
    "        mGridCoord = self.mGridCoord\n",
    "\n",
    "        Ncoords = mGridCoord.shape[0]\n",
    "        Ntaylor = 1 + Ncoords # + (Ncoords*(Ncoords+1))//2\n",
    "        self.Ntaylor = Ntaylor\n",
    "\n",
    "        M = mGridPmt.shape[0]\n",
    "        mGridI = np.reshape(np.repeat(np.arange(M),M), (M,M))\n",
    "        mGridJ = mGridI.transpose()\n",
    "\n",
    "        Nshift = 2*pad+1\n",
    "        dij = np.reshape(np.repeat(np.arange(-pad,pad+1),Nshift), (Nshift,Nshift));\n",
    "        dis = dij.flatten()\n",
    "        djs = dij.transpose().flatten();\n",
    "        Nsites = dis.size;\n",
    "        \n",
    "        self.dis = dis\n",
    "        self.djs = djs\n",
    "        self.Nsites = Nsites\n",
    "\n",
    "        mGridPmtpad = geometricPad(torch.tensor(mGridPmt).unsqueeze(0).unsqueeze(0), pad).squeeze(0).squeeze(0).numpy();\n",
    "        mGridIpad = geometricPad(torch.tensor(mGridI).unsqueeze(0).unsqueeze(0), pad).squeeze(0).squeeze(0).numpy();\n",
    "        mGridJpad = geometricPad(torch.tensor(mGridJ).unsqueeze(0).unsqueeze(0), pad).squeeze(0).squeeze(0).numpy();\n",
    "\n",
    "        mGridCoordPad = geometricPad(torch.tensor(mGridCoord).unsqueeze(0), pad).squeeze(0).numpy();\n",
    "\n",
    "\n",
    "        mGridAA2 = np.zeros((M,M,Nshift,Nshift,Ntaylor))\n",
    "\n",
    "        pmtI1 = mGridPmt[:,:]; # index in tank.pmt\n",
    "\n",
    "        x      = np.zeros((Ncoords,M,M,Nsites));     # coordinates for each direction\n",
    "        skip   = np.zeros((M,M,Nsites), dtype=bool); # for each direction whether we should skip or not\n",
    "        processedPmtI2s = np.zeros((M,M,Nsites), dtype=int) - 1;\n",
    "\n",
    "        for diji in range(Nsites):\n",
    "            di = dis[diji]\n",
    "            dj = djs[diji]\n",
    "            # i2 = mGridIpad[pad+di:pad+M+di,pad+dj:pad+M+dj];\n",
    "            # j2 = mGridJpad[pad+di:pad+M+di,pad+dj:pad+M+dj];\n",
    "            pmtI2 = mGridPmtpad[(pad+di):(pad+M+di),(pad+dj):(pad+M+dj)];\n",
    "            processedPmtI2s[:,:,diji] = pmtI2;\n",
    "\n",
    "            skip[:,:,diji] = np.logical_or(np.equal(pmtI1,-1), np.equal(pmtI2,-1));\n",
    "            for prev_diji in range(diji):\n",
    "                skip[:,:,diji] = np.logical_or(skip[:,:,diji], np.equal(processedPmtI2s[:,:,prev_diji],pmtI2));\n",
    "\n",
    "            x[:,:,:,diji] = mGridCoordPad[:,(pad+di):(pad+M+di),(pad+dj):(pad+M+dj)] - mGridCoord; # \n",
    "\n",
    "        # x: (Ncoords,M,M,Nsites)\n",
    "        # print('x.shape:', x.shape)\n",
    "        # X: (Ntaylor,M,M,Nsites)\n",
    "        Xlist = [np.expand_dims(np.ones(x.shape[1:]),0),x]\n",
    "        #for i in range(Ncoords):\n",
    "        #    Xlist.append(np.stack([x[i,:,:,:] for j in range(Ncoords-i)]) * x[i:,:,:,:])\n",
    "        X = np.concatenate(Xlist)\n",
    "        # print('X.shape:', X.shape)\n",
    "        X = np.where(np.expand_dims(skip, 0), 0., X);\n",
    "\n",
    "        # regularization strength\n",
    "        Lambda = self.Lambda; # this should be somewhat smaller than typical values of x\n",
    "\n",
    "        MA = np.zeros((M,M,Nsites,Nsites));\n",
    "        for k in range(Nsites):\n",
    "            for j in range(Nsites):\n",
    "                MA[:,:,k,j] = np.add(np.sum(X[:,:,:,k]*X[:,:,:,j], axis=0), Lambda**2 * (k==j));\n",
    "            # clear skip entries\n",
    "            MA[:,:,k,:] = np.where(np.expand_dims(skip[:,:,k],-1), 0., MA[:,:,k,:]);\n",
    "            MA[:,:,:,j] = np.where(np.expand_dims(skip[:,:,j],-1), 0., MA[:,:,:,j]);\n",
    "            # set diagonal:\n",
    "            MA[:,:,k,k] = np.where(skip[:,:,k], 1., MA[:,:,k,k]);\n",
    "\n",
    "        #print('MA.shape:',MA.shape)\n",
    "        #print(MA[0,1,:,:])\n",
    "        #print(np.linalg.inv(MA[0,1,:,:]))\n",
    "        print(\"starting inversion... (may take a few seconds)\")\n",
    "        AA = np.einsum('hwsi,thwi->thws', np.linalg.inv(MA), X)\n",
    "        print(\"done\")\n",
    "        # print('AA.shape:', AA.shape)\n",
    "        return torch.tensor(AA, dtype=torch.float)\n",
    "    \n",
    "    def taylorize(self, inputs):\n",
    "        Batches = inputs.shape[0]\n",
    "        ChIn    = inputs.shape[1]\n",
    "        Height  = inputs.shape[2]\n",
    "        Width   = inputs.shape[3]\n",
    "        tGridAA = self.AA\n",
    "\n",
    "        inputsPad = geometricPad(inputs, self.pad); # (B,C,W,H)\n",
    "        # tayloredInput = inputs.new_zeros((Batches, self.Ntaylor, ChIn, Height, Width))\n",
    "        \n",
    "        # we use new_zeros and set diagonal to 1, because there is no new_eye method\n",
    "        rotMatrix = inputs.new_zeros(self.Ntaylor, self.Ntaylor)\n",
    "        for d in range(self.Ntaylor):\n",
    "            rotMatrix[d,d] = 1.\n",
    "        \n",
    "        for startIndex in self.vectorStartIndices:\n",
    "            startIndex = 1 + startIndex # the 1 is offset due to scalar\n",
    "            d_x = 0\n",
    "            d_y = 1\n",
    "            rotMatrix[startIndex+d_x,startIndex+d_x] =  np.cos(self.rotPhi);\n",
    "            rotMatrix[startIndex+d_x,startIndex+d_y] =  np.sin(self.rotPhi); # this is an inverse rotation because w is covariant\n",
    "            rotMatrix[startIndex+d_y,startIndex+d_x] = -np.sin(self.rotPhi);\n",
    "            rotMatrix[startIndex+d_y,startIndex+d_y] =  np.cos(self.rotPhi);\n",
    "        \n",
    "        #print('tGridAA.shape:', tGridAA.shape)\n",
    "        #print('tayloredInput.shape:', tayloredInput.shape)\n",
    "        #print('inputsPad.shape:', inputsPad.shape)\n",
    "        # for k in range(self.Nsites):\n",
    "        #     # shift grid according to position (without copying the data)\n",
    "        #     inputsPadD1 = torch.narrow(inputsPad,   2, self.pad+int(self.dis[k]), Height)\n",
    "        #     inputsPadD2 = torch.narrow(inputsPadD1, 3, self.pad+int(self.djs[k]), Width )\n",
    "        #     #print('inputsPadD2.shape:', inputsPadD2.shape)\n",
    "        #     # fill\n",
    "        #     # d,e: directional index (Ntaylor)\n",
    "        #     # i,j: grid indices\n",
    "        #     # b: event in batch\n",
    "        #     # c: channel\n",
    "        #     \n",
    "        #     # out = torch.add(tayloredInput, torch.einsum('dij,bcij->bdcij', tGridAA[:,:,:,k], inputsPadD2))\n",
    "        #     # tayloredInput = out\n",
    "        #     \n",
    "        #     # dij,bcij->bdcij\n",
    "        #     tayloredInput += tGridAA[:,:,:,k].view(1, self.Ntaylor, 1, Height, Width) * inputsPadD2.view(Batches, 1, ChIn, Height, Width)\n",
    "        \n",
    "        # inputsPad      = (B, ChIn, Height+1, Width+1)\n",
    "        #print('inputsPad', inputsPad.shape, inputsPad.stride())\n",
    "        inputsUnfolded = torch.nn.functional.unfold(inputsPad, kernel_size=(2*self.pad+1,2*self.pad+1))\n",
    "        #print('inputsUnfolded', inputsUnfolded.shape, inputsUnfolded.stride())\n",
    "        #print('tGridAA', tGridAA.shape, tGridAA.stride())\n",
    "        # inputsUnfolded = (B, C*Nsites, Height*Width)\n",
    "        # tGridAA = (Ntaylor, Height, Width, Nsites)\n",
    "        inputsUnfolded = inputsUnfolded.view(Batches, ChIn, self.Nsites, Height, Width)\n",
    "        #print('inputsUnfolded after view', inputsUnfolded.shape, inputsUnfolded.stride())\n",
    "        tayloredInput = torch.einsum('dijs,bcsij->bdcij', tGridAA, inputsUnfolded)\n",
    "        #print('tayloredInput', tayloredInput.shape, tayloredInput.stride())\n",
    "        #print('target', Batches, self.Ntaylor*ChIn, Height, Width)\n",
    "        \n",
    "        if self.rotPhi != 0.:\n",
    "            org = tayloredInput\n",
    "            tayloredInput = torch.einsum('bdcij,ed->becij', org, rotMatrix).contiguous()\n",
    "            #tayloredInput = out\n",
    "        \n",
    "        #print('tayloredInput.shape', tayloredInput.shape, 'to', (Batches, self.Ntaylor*ChIn, Height, Width))\n",
    "        # this contiguous() step is only needed when using unfold (problem is what exactly einsum does)\n",
    "        return tayloredInput.contiguous().view(Batches, self.Ntaylor*ChIn, Height, Width)\n",
    "    \n",
    "    def pool2d(self, kernel_size, stride=None, padding=0, pad=-1, Lambda=-1.):\n",
    "        \"\"\"\n",
    "        Return new Taylorizer that can be used for the result of a 2d pool operation with given kernel size.\n",
    "        kernel_size: e.g. for 2x2 max-pooling use (2,2)\n",
    "        stride: defaults to kernel_size\n",
    "        padding: the padding for the pool2d operation (not passed to child taylor). default: 0\n",
    "        pad:    passed to child. if unset (-1)  use same as current\n",
    "        Lambda: passed to child. if unset (-1.) use same as current\n",
    "        \"\"\"\n",
    "        if stride is None:\n",
    "            stride = kernel_size\n",
    "        mGridPmt1    = torch.nn.functional.max_pool2d(torch.tensor(self.mGridPmt  )   .to(torch.float).unsqueeze(0), kernel_size=kernel_size, stride=stride, padding=padding).squeeze(0).to(torch.long).numpy()\n",
    "        mGridFilled1 = torch.nn.functional.avg_pool2d(torch.tensor((self.mGridPmt >= 0).astype(float)).unsqueeze(0), kernel_size=kernel_size, stride=stride, padding=padding).squeeze(0).numpy()\n",
    "        \n",
    "        mGridCoord1 = torch.nn.functional.avg_pool2d(torch.tensor(self.mGridCoord), kernel_size=kernel_size, stride=stride, padding=padding).numpy()\n",
    "        mGridCoord1 /= mGridFilled1 + 1e-6 # correct average for missing positions. the 1e-6 is to prevent division by zero\n",
    "        \n",
    "        if (pad < 0):\n",
    "            pad = self.pad\n",
    "        if (Lambda < 0.):\n",
    "            Lambda = self.Lambda\n",
    "        \n",
    "        return Taylorizer(mGridPmt1, mGridCoord1, self.vectorStartIndices, pad=pad, Lambda=Lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mGridPmt = f['mGridPmt'][()]\n",
    "# we rotate xyz -> zxy because in IWCD tank, the y axis points along the tank axis\n",
    "mGridCoord = np.stack([\n",
    "    f['mGridZ'][()]/20.,\n",
    "    f['mGridX'][()]/20.,\n",
    "    f['mGridY'][()]/20.,\n",
    "    #f['mGridDirZ'][()],\n",
    "    #f['mGridDirX'][()],\n",
    "    #f['mGridDirY'][()]\n",
    "])\n",
    "mGridVectorStartIndices = [0]\n",
    "#mGridVectorStartIndices = [0,3]\n",
    "\n",
    "taylor0 = Taylorizer(mGridPmt, mGridCoord, mGridVectorStartIndices, pad=1, Lambda=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filters = torch.randn(Cout,Cin,Hflt,Wflt);\n",
    "inputs = torch.tensor(event_data[0:5,:,:,0]).unsqueeze(1);\n",
    "tayloredInput = taylor0(inputs)\n",
    "\n",
    "for ev in range(tayloredInput.shape[0]):\n",
    "    print(ev)\n",
    "    taylorTitles = ['1','x','y','z'] #,'dir_x','dir_y','dir_z']\n",
    "    fig, axes = plt.subplots(1, taylor0.Ntaylor, figsize=(16,8),facecolor='w')\n",
    "    for itaylor in range(taylor0.Ntaylor):\n",
    "        rms = torch.std(tayloredInput[ev,itaylor,:,:])\n",
    "        ext = torch.max(torch.abs(tayloredInput[ev,itaylor,:,:]))\n",
    "        imshowRdBu(axes[itaylor], tayloredInput[ev,itaylor,:,:]); axes[itaylor].set_title('%s\\nstd=%.2f\\nmax=%.2f' % (taylorTitles[itaylor],rms,ext))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class H5Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, files, transform=None, flavour=None, limit_num_files=0, start_fraction=0., use_fraction=1.0):\n",
    "        \"\"\"                                                                                                                                             \n",
    "        Args: data_dirs ... a list of data directories to find files (up to 10 files read from each dir)                                                \n",
    "              transform ... a function applied to pre-process data                                                                                      \n",
    "              flavour ..... a string that is required to be present in the filename                                                                     \n",
    "              limit_num_files ... an integer limiting number of files to be taken per data directory                                                    \n",
    "              start_fraction ... a floating point fraction (0.0=>1.0) to specify which entry to start reading (per file)                                \n",
    "              use_fraction ..... a floating point fraction (0.0=>1.0) to specify how much fraction of a file to be read out (per file)                  \n",
    "        \"\"\"\n",
    "        self._transform = transform\n",
    "        self._files = []\n",
    "\n",
    "        # Check input fractions makes sense                                                                                                             \n",
    "        assert start_fraction >= 0. and start_fraction < 1.\n",
    "        assert use_fraction > 0. and use_fraction <= 1.\n",
    "        assert (start_fraction + use_fraction) <= 1.\n",
    "\n",
    "        # Load files (up to 10) from each directory in data_dirs list                                                                                   \n",
    "        # for d in data_dirs:\n",
    "        #     file_list = [ os.path.join(d,f) for f in os.listdir(d) if flavour is None or flavour in f ]\n",
    "        #     if limit_num_files: file_list = file_list[0:limit_num_files]\n",
    "        #     self._files += file_list\n",
    "        self._files = files\n",
    "\n",
    "        self._file_handles = [None] * len(self._files)\n",
    "        self._event_to_file_index  = []\n",
    "        self._event_to_entry_index = []\n",
    "        import h5py\n",
    "        for file_index, file_name in enumerate(self._files):\n",
    "            f = h5py.File(file_name,mode='r')\n",
    "            data_size = f['event_data'].shape[0]\n",
    "            start_entry = int(start_fraction * data_size)\n",
    "            num_entries = int(use_fraction * data_size)\n",
    "            self._event_to_file_index += [file_index] * num_entries\n",
    "            self._event_to_entry_index += range(start_entry, start_entry+num_entries)\n",
    "            f.close()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._event_to_file_index)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        file_index = self._event_to_file_index[idx]\n",
    "        entry_index = self._event_to_entry_index[idx]\n",
    "        if self._file_handles[file_index] is None:\n",
    "            import h5py\n",
    "            self._file_handles[file_index] = h5py.File(self._files[file_index],mode='r')\n",
    "        fh = self._file_handles[file_index]\n",
    "        label = fh['labels'][entry_index]\n",
    "        #labelTranslation = [-1,0,1,-1,2] # make sure that labels 1,2,4 get values 0,1,2\n",
    "        if label == 1: # electron\n",
    "            label = 0\n",
    "        elif label == 2: # muon\n",
    "            label = 1\n",
    "        elif label == 0: # gamma\n",
    "            label = 2\n",
    "        else:\n",
    "            print('Unknown label', label, 'for entry_index', entry_index, 'treating as label=0')\n",
    "            label = 0\n",
    "        \n",
    "        # try:\n",
    "        #     label = labelTranslation[label]\n",
    "        # except IndexError:\n",
    "        #     print('IndexError at entry', entry_index, 'label in file was', fh['labels'][entry_index], 'translations are', labelTranslation)\n",
    "        #     raise\n",
    "        \n",
    "        event_data = fh['event_data'][entry_index]\n",
    "        # convert event data to complex rep\n",
    "        evQ = event_data[:,:,0]\n",
    "        evT = event_data[:,:,1]\n",
    "        evR = np.power(evQ,0.25);\n",
    "        evA = 2*np.pi*(evT-960.)/(1900.-600.)\n",
    "        evX = evR*np.cos(evA);\n",
    "        evY = evR*np.sin(evA);\n",
    "        return np.stack([evX,evY],2),label,idx,fh['positions'][entry_index,0],fh['directions'][entry_index,0],np.sum(fh['energies'][entry_index,:])\n",
    "        # the ,0 in positions and directions selects the pos/dir for the first track (in case of gamma)\n",
    "        # thus positions and directions are just np arrays with 3 elements\n",
    "\n",
    "def HKCollate(batch):\n",
    "    data  = np.stack([sample[0] for sample in batch])\n",
    "    label = [sample[1] for sample in batch]\n",
    "    idx   = [sample[2] for sample in batch]\n",
    "    pos   = np.stack([sample[3] for sample in batch])\n",
    "    direc = np.stack([sample[4] for sample in batch])\n",
    "    ene   = np.stack([sample[5] for sample in batch])\n",
    "    return data,label,idx,pos,direc,ene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcdir = '/home/lukasb/watchmal/data_ssd/IWCDmPMT_4pi_full_tank/h5_topo'\n",
    "pnames = ('e-','mu-')\n",
    "files = ['%s/%s/IWCDmPMT_4pi_full_tank_%s_E0to1000MeV_unif-pos-R371-y521cm_4pi-dir_3000evts_%d.h5' % (mcdir,pname,pname,bch+100) for bch in range(60) for pname in pnames]\n",
    "\n",
    "train_ds = H5Dataset(files,start_fraction=0.0,use_fraction=0.9);\n",
    "test_ds  = H5Dataset(files,start_fraction=0.9,use_fraction=0.1);\n",
    "\n",
    "# for training\n",
    "train_loader=DataLoader(train_ds,batch_size=32,shuffle=True,num_workers=4,collate_fn=HKCollate)\n",
    "# for validation\n",
    "test_loader =DataLoader( test_ds,batch_size=64,shuffle=True,num_workers=2,collate_fn=HKCollate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# based on https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py\n",
    "\n",
    "class TaylorBNConv2d(torch.nn.Module):\n",
    "    def __init__(self, Cin, Cout, taylor, stride=1, bias=True):\n",
    "        super(TaylorBNConv2d, self).__init__()\n",
    "        self.stride = stride\n",
    "        self.Cin = Cin\n",
    "        self.Cout = Cout\n",
    "        self.taylor = taylor\n",
    "        #self.taylor_substitue = torch.nn.Conv2d(Cin, Cin*taylor.Ntaylor, kernel_size=3, stride=1, padding=0, bias=False)\n",
    "        self.strider = torch.nn.MaxPool2d(kernel_size=1, stride=stride, padding=0)\n",
    "        #self.scales = torch.nn.Parameter(torch.ones(taylor.Ntaylor))\n",
    "        self.conv2d = torch.nn.Conv2d(Cin*taylor.Ntaylor,Cout, kernel_size=1, stride=1, padding=0, bias=bias)\n",
    "        self.batchNorm2d = torch.nn.BatchNorm2d(Cout)\n",
    " \n",
    "    def forward(self, x):\n",
    "        #x = self.taylor_substitue(geometricPad(x,1))\n",
    "        x = self.taylor(x)\n",
    "        if self.stride > 1:\n",
    "            x = self.strider(x)\n",
    "        #x = self.taylor(x)\n",
    "        #x = self.bn1(x)\n",
    "        #if self.stride > 1:\n",
    "        #    orgshape = x.shape\n",
    "        #    newshape = (\n",
    "        #        x.shape[0],\n",
    "        #        x.shape[1],\n",
    "        #        int(np.ceil(x.shape[2]/self.stride)),\n",
    "        #        int(np.ceil(x.shape[3]/self.stride))\n",
    "        #    )\n",
    "        #    newstride = (1,1,self.stride,self.stride)\n",
    "        #    x = torch.as_strided(x, newshape, newstride)\n",
    "        #    #print('TaylorBNConv2d::forward: stride operation changed size from', orgshape, 'to', x.shape)\n",
    "        \n",
    "        # # scale the taylored dimensions\n",
    "        # orgshape = x.shape\n",
    "        # x = x.view(x.shape[0], self.taylor.Ntaylor, self.Cin, x.shape[2], x.shape[3])\n",
    "        # scales = self.scales.view(1, self.taylor.Ntaylor, 1, 1, 1).expand(x.shape)\n",
    "        # x *= scales\n",
    "        # x = x.view(orgshape)\n",
    "        \n",
    "        x = self.conv2d(x)\n",
    "        x = self.batchNorm2d(x)\n",
    "        return x\n",
    "    \n",
    "    # the default implementation of cpu() and cuda() only passes this along to nn.Module instances\n",
    "    # since self.AA is a tensor, we need to take care of it ourselves\n",
    "    \n",
    "    def cpu(self):\n",
    "        super(TaylorBNConv2d, self).cpu()\n",
    "        self.taylor.cpu()\n",
    "        return self\n",
    "    \n",
    "    def cuda(self):\n",
    "        super(TaylorBNConv2d, self).cuda()\n",
    "        self.taylor.cuda()\n",
    "        return self\n",
    "\n",
    "# second order derivative (by stacking two first order derviatives)\n",
    "class SecTaylorBNConv2d(torch.nn.Module):\n",
    "    def __init__(self, Cin, Cout, taylor, stride=1, bias=True):\n",
    "        super(SecTaylorBNConv2d, self).__init__()\n",
    "        \n",
    "        self.tbc1 = TaylorBNConv2d(Cin,  Cout, taylor, stride=1,     bias=bias)\n",
    "        self.tbc2 = TaylorBNConv2d(Cout, Cout, taylor, stride=stride, bias=bias)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.tbc1(x)\n",
    "        x = self.tbc2(x)\n",
    "        return x\n",
    "    \n",
    "    # the default implementation of cpu() and cuda() only passes this along to nn.Module instances\n",
    "    # since self.AA is a tensor, we need to take care of it ourselves\n",
    "    \n",
    "    def cpu(self):\n",
    "        super(SecTaylorBNConv2d, self).cpu()\n",
    "        self.tbc1.cpu()\n",
    "        self.tbc2.cpu()\n",
    "        return self\n",
    "    \n",
    "    def cuda(self):\n",
    "        super(SecTaylorBNConv2d, self).cuda()\n",
    "        self.tbc1.cuda()\n",
    "        self.tbc2.cuda()\n",
    "        return self\n",
    "\n",
    "\n",
    "class BasicTaylorBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, taylor1, taylor2, stride=1):\n",
    "        super(BasicTaylorBlock, self).__init__()\n",
    "        \n",
    "        self.tbc1 = TaylorBNConv2d(in_planes, planes, taylor1, stride=stride, bias=False)\n",
    "        self.tbc2 = TaylorBNConv2d(   planes, planes, taylor2, stride=1,      bias=False)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.tbc1(x))\n",
    "        out = F.relu(self.tbc2(out) + self.shortcut(x))\n",
    "        return out\n",
    "    \n",
    "    # standard implementation only calles cpu/gpu on the model's [parameters]\n",
    "    # since we want it on the model itself, redefine\n",
    "    \n",
    "    def cpu(self):\n",
    "        super(BasicTaylorBlock, self).cpu()\n",
    "        self.tbc1.cpu()\n",
    "        self.tbc2.cpu()\n",
    "        return self\n",
    "    \n",
    "    def cuda(self):\n",
    "        super(BasicTaylorBlock, self).cuda()\n",
    "        self.tbc1.cuda()\n",
    "        self.tbc2.cuda()\n",
    "        return self\n",
    "\n",
    "\n",
    "class ResNetTaylor(nn.Module):\n",
    "    def __init__(self, block, num_blocks, taylor, num_classes=3):\n",
    "        super(ResNetTaylor, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        overall_in_planes = 2 ## this has been modified to include time\n",
    "        self.pad = 1\n",
    "        \n",
    "        # (1,126,126)\n",
    "        taylor1 = taylor\n",
    "        taylor2 = taylor1.pool2d(3, stride=2, padding=1)\n",
    "        taylor3 = taylor2.pool2d(3, stride=2, padding=1)\n",
    "        taylor4 = taylor3.pool2d(3, stride=2, padding=1)\n",
    "        taylor5 = taylor4.pool2d(3, stride=2, padding=1)\n",
    "        taylor6 = taylor5.pool2d(3, stride=2, padding=1)\n",
    "        \n",
    "        #print('ResNetTaylor: taylor1.AA', taylor1.AA.shape)\n",
    "        #print('ResNetTaylor: taylor2.AA', taylor2.AA.shape)\n",
    "        #print('ResNetTaylor: taylor3.AA', taylor3.AA.shape)\n",
    "        #print('ResNetTaylor: taylor4.AA', taylor4.AA.shape)\n",
    "        #print('ResNetTaylor: taylor5.AA', taylor5.AA.shape)\n",
    "        \n",
    "        self.down1 = TaylorBNConv2d(overall_in_planes, 32, taylor1, stride=2, bias=False)\n",
    "        \n",
    "        # outsize = floor[(126+2*(pad=1)-(kernel_size=3))/stride + 1]\n",
    "        # so for stride=1 we have 126,\n",
    "        #              =2          63 = floor(63.5)\n",
    "        # the padding is done by geometricPad\n",
    "        # thus (16,63,63)\n",
    "        \n",
    "        self.down2 = TaylorBNConv2d(32, 64, taylor2, stride=2, bias=False)\n",
    "        # outsize = floor[(63+2*(pad=1)-(kernel_size=3))/stride + 1]\n",
    "        # so for stride=1 we have 63,\n",
    "        #              =2         32\n",
    "        # thus (64,32,32)\n",
    "        \n",
    "        self.layer1 = self._make_layer(block,  64, num_blocks[0], taylor3, taylor3, stride=1)\n",
    "        #      (64,32,32)\n",
    "        self.layer2 = self._make_layer(block,  64, num_blocks[1], taylor3, taylor4, stride=2)\n",
    "        #      (64,16,16)\n",
    "        self.layer3 = self._make_layer(block,  64, num_blocks[2], taylor4, taylor5, stride=2)\n",
    "        #      (64, 8, 8)\n",
    "        self.layer4 = self._make_layer(block, 128, num_blocks[3], taylor5, taylor6, stride=2)\n",
    "        #      (128,4, 4)\n",
    "        self.convbn5 = nn.Sequential(\n",
    "            nn.Conv2d(128,128,kernel_size=4,stride=1,padding=0,bias=False),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "        #      (128,1, 1)\n",
    "        # transform to 128, then\n",
    "        self.linear1 = nn.Linear(int(128*block.expansion),   int(128*block.expansion/2))\n",
    "        self.linear2 = nn.Linear(int(128*block.expansion/2), int(128*block.expansion/4))\n",
    "        self.linear3 = nn.Linear(int(128*block.expansion/4), int(128*block.expansion/8))\n",
    "        self.linear4 = nn.Linear(int(128*block.expansion/8), num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, taylor, taylor_substride, stride):\n",
    "        strides = [(stride,taylor,taylor_substride)] + [(1,taylor_substride,taylor_substride)]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride,pretaylor,posttaylor in strides:\n",
    "            layers.append(block(self.in_planes, planes, pretaylor, posttaylor, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print('x', x.shape)\n",
    "        out = F.relu(self.down1(x))\n",
    "        #print('self.down1', out.shape)\n",
    "        out = F.relu(self.down2(out))\n",
    "        #print('self.down2', out.shape)\n",
    "        out = self.layer1(out)\n",
    "        #print('self.layer1', out.shape)\n",
    "        out = self.layer2(out)\n",
    "        #print('self.layer2', out.shape)\n",
    "        out = self.layer3(out)\n",
    "        #print('self.layer3', out.shape)\n",
    "        out = self.layer4(out)\n",
    "        out = F.relu(self.convbn5(out))\n",
    "        #out = F.avg_pool2d(out, out.shape[2:4])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.relu(self.linear1(out))\n",
    "        out = F.relu(self.linear2(out))\n",
    "        out = F.relu(self.linear3(out))\n",
    "        out = self.linear4(out)\n",
    "        return out\n",
    "    \n",
    "    # standard implementation only calles cpu/gpu on the model's [parameters]\n",
    "    # since we want it on the model itself, redefine\n",
    "    \n",
    "    def cpu(self):\n",
    "        super(ResNetTaylor, self).cpu()\n",
    "        self.down1.cpu()\n",
    "        self.down2.cpu()\n",
    "        self.layer1.apply(lambda m: m.cpu())\n",
    "        self.layer2.apply(lambda m: m.cpu())\n",
    "        self.layer3.apply(lambda m: m.cpu())\n",
    "        self.layer4.apply(lambda m: m.cpu())\n",
    "        return self\n",
    "    \n",
    "    def cuda(self):\n",
    "        super(ResNetTaylor, self).cuda()\n",
    "        self.down1.cuda()\n",
    "        self.down2.cuda()\n",
    "        self.layer1.apply(lambda m: m.cuda())\n",
    "        self.layer2.apply(lambda m: m.cuda())\n",
    "        self.layer3.apply(lambda m: m.cuda())\n",
    "        self.layer4.apply(lambda m: m.cuda())\n",
    "        return self\n",
    "\n",
    "def ResNetTaylor18(taylor, num_classes):\n",
    "    return ResNetTaylor(BasicTaylorBlock, [2, 2, 2, 2], taylor, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(blob,train=True):\n",
    "    \"\"\"\n",
    "       Args: blob should have attributes, net, criterion, softmax, data, label\n",
    "       Returns: a dictionary of predicted labels, softmax, loss, and accuracy\n",
    "    \"\"\"\n",
    "    with torch.set_grad_enabled(train):\n",
    "        # Prediction\n",
    "        data = torch.as_tensor(blob.data).cuda()#[torch.as_tensor(d).cuda() for d in blob.data]\n",
    "        data = data.permute(0,3,1,2)\n",
    "        prediction = blob.net(data)\n",
    "        # Training\n",
    "        loss,acc=-1,-1\n",
    "        if blob.label is not None:\n",
    "            label = torch.as_tensor(blob.label).type(torch.LongTensor).cuda()#[torch.as_tensor(l).cuda() for l in blob.label]\n",
    "            #label = torch.as_tensor(blob.label).type(torch.FloatTensor).cuda()#[torch.as_tensor(l).cuda() for l in blob.label]\n",
    "            label.requires_grad = False\n",
    "            loss = blob.criterion(prediction,label)\n",
    "        blob.loss = loss\n",
    "        \n",
    "        softmax    = blob.softmax(prediction).cpu().detach().numpy()\n",
    "        prediction = torch.argmax(prediction,dim=-1)\n",
    "        accuracy   = (prediction == label).sum().item() / float(prediction.nelement())\n",
    "        # mse        = blob.mse(prediction,label).cpu().detach().numpy()\n",
    "        # accuracy   = np.sqrt(mse.mean()).item()\n",
    "        prediction = prediction.cpu().detach().numpy()\n",
    "        \n",
    "        return {'prediction' : prediction,\n",
    "                'softmax'    : softmax,\n",
    "                'loss'       : loss.cpu().detach().item(),\n",
    "                'accuracy'   : accuracy}\n",
    "\n",
    "def backward(blob):\n",
    "    blob.optimizer.zero_grad()  # Reset gradients accumulation\n",
    "    blob.loss.backward()\n",
    "    blob.optimizer.step()\n",
    "\n",
    "def save_state(blob):\n",
    "    # Output file name\n",
    "    filename = '%s-%d.ckpt' % (blob.prefix, blob.iteration)\n",
    "    # Save parameters\n",
    "    # 0+1) iteration counter + optimizer state => in case we want to \"continue training\" later\n",
    "    # 2) network weight\n",
    "    torch.save({\n",
    "        'global_epoch': blob.epoch,\n",
    "        'global_step': blob.iteration,\n",
    "        'optimizer': blob.optimizer.state_dict(),\n",
    "        'state_dict': blob.net.state_dict()\n",
    "        }, filename)\n",
    "    return filename\n",
    "\n",
    "def restore_state(blob, iteration):\n",
    "    # Open a file in read-binary mode\n",
    "    weight_file = '%s-%d.ckpt' % (blob.prefix, iteration)\n",
    "    with open(weight_file, 'rb') as f:\n",
    "        # torch interprets the file, then we can access using string keys\n",
    "        checkpoint = torch.load(f)\n",
    "        # load network weights\n",
    "        blob.net.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "        # if optimizer is provided, load the state of the optimizer\n",
    "        if blob.optimizer is not None:\n",
    "            blob.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        # load iteration count\n",
    "        blob.epoch     = checkpoint['global_epoch']\n",
    "        blob.iteration = checkpoint['global_step']\n",
    "\n",
    "# weight_file = save_state(blob, '20190819-02-DeepTaylor-01-BatchNorm')\n",
    "# print('Saved to', weight_file)\n",
    "\n",
    "\n",
    "# # Recreate the network (i.e. initialize)\n",
    "# blob.net=LeNet().cuda()\n",
    "# # Get one batch of data to test\n",
    "# blob.data, blob.label = next(iter(train_loader))\n",
    "# # Run forward function\n",
    "# res = forward(blob,True)\n",
    "# # Report\n",
    "# print('Accuracy:',res['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# # Restore the state\n",
    "# restore_state(weight_file,blob)\n",
    "# # Run the forward function\n",
    "# res = forward(blob,True)\n",
    "# # Report\n",
    "# print('Accuracy',res['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLOB:\n",
    "    pass\n",
    "blob=BLOB()\n",
    "blob.net       = ResNetTaylor18(taylor0, 2).cuda() # construct ResNet for 2 class classification, use GPU\n",
    "# just the training weights is ~700MiB\n",
    "blob.criterion = torch.nn.CrossEntropyLoss() # use softmax loss to define an error\n",
    "blob.optimizer = torch.optim.Adam(blob.net.parameters()) # use Adam optimizer algorithm\n",
    "blob.softmax   = torch.nn.Softmax(dim=1) # not for training, but softmax score for each class\n",
    "blob.prefix    = '20200708-14-IWCD-SmallResNetTaylor-timeCmplx-01-e-mu'\n",
    "blob.epoch     = 0.\n",
    "blob.iteration = 0\n",
    "blob.data      = None # data for training/analysis\n",
    "blob.label     = None # label for training/analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(blob.net.down1.conv2d.parameters()))\n",
    "# print(blob.net.down2.scales)\n",
    "# print(blob.net.layer1[0].tbc1.scales)\n",
    "# print(blob.net.layer1[0].tbc2.scales)\n",
    "# print(blob.net.layer1[1].tbc1.scales)\n",
    "# print(blob.net.layer1[1].tbc2.scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from utils.utils import progress_bar, CSVData\n",
    "\n",
    "blob.train_log, blob.test_log = CSVData('%s-log_train.csv' % blob.prefix), CSVData('%s-log_test.csv' % blob.prefix)\n",
    "\n",
    "# Define train period. \"epoch\" = N image consumption where N is the total number of train samples.\n",
    "TRAIN_EPOCH=4.0\n",
    "# Set the network to training mode\n",
    "blob.net.train()\n",
    "\n",
    "# Start training\n",
    "while int(blob.epoch+0.5) < TRAIN_EPOCH:\n",
    "    print('Epoch',int(blob.epoch+0.5),'Starting @',time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "    # Create a progress bar for this epoch\n",
    "    progress = display(progress_bar(0,len(train_loader)),display_id=True)\n",
    "    # Loop over data samples and into the network forward function\n",
    "    for i,data in enumerate(train_loader):\n",
    "        # Data and label\n",
    "        blob.data,blob.label = data[0:2]\n",
    "        # Call forward: make a prediction & measure the average error\n",
    "        res = forward(blob,True)\n",
    "        # Call backward: backpropagate error and update weights\n",
    "        backward(blob)\n",
    "        # Epoch update\n",
    "        blob.epoch += 1./len(train_loader)\n",
    "        blob.iteration += 1\n",
    "        \n",
    "        #\n",
    "        # Log/Report\n",
    "        #\n",
    "        # Record the current performance on train set\n",
    "        blob.train_log.record(['iteration','epoch','accuracy','loss'],[blob.iteration,blob.epoch,res['accuracy'],res['loss']])\n",
    "        blob.train_log.write()\n",
    "        # once in a while, report\n",
    "        if i==0 or (i+1)%10 == 0:\n",
    "            message = '... Iteration %d ... Epoch %1.2f ... Loss %1.3f ... Accuracy %1.3f' % (blob.iteration,blob.epoch,res['loss'],res['accuracy'])\n",
    "            progress.update(progress_bar((i+1),len(train_loader),message))\n",
    "        # more rarely, run validation\n",
    "        if (i+1)%40 == 0:\n",
    "            with torch.no_grad():\n",
    "                blob.net.eval()\n",
    "                test_data = next(iter(test_loader))\n",
    "                blob.data,blob.label = test_data[0:2]\n",
    "                res = forward(blob,False)\n",
    "                blob.test_log.record(['iteration','epoch','accuracy','loss'],[blob.iteration,blob.epoch,res['accuracy'],res['loss']])\n",
    "                blob.test_log.write()\n",
    "            blob.net.train()\n",
    "        # even more rarely, save state\n",
    "        if (i+1)%400 == 0:\n",
    "            with torch.no_grad():\n",
    "                blob.net.eval()\n",
    "                save_state(blob)\n",
    "            blob.net.train()\n",
    "        if blob.epoch >= TRAIN_EPOCH:\n",
    "            break\n",
    "    message = '... Iteration %d ... Epoch %1.2f ... Loss %1.3f ... Accuracy %1.3f' % (blob.iteration,blob.epoch,res['loss'],res['accuracy'])\n",
    "    #print(message)\n",
    "    progress.update(progress_bar((i+1),len(train_loader),message))\n",
    "\n",
    "blob.test_log.close()\n",
    "blob.train_log.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "memory footprint is about 2803MiB /  7979MiB with 32 events/batch (so about twice that of ResNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to follow status in R\n",
    "\n",
    "def Rcode:\n",
    "    prefix=\"20200708-14-IWCD-SmallResNetTaylor-timeCmplx-01-e-mu\"\n",
    "\n",
    "    ma <- function(x, n = 5){filter(x, rep(1 / n, n), sides = 2)}\n",
    "\n",
    "    maxepochs = 4.\n",
    "    dfTrain = rbind(\n",
    "      read.csv(sprintf(\"/home/lukasb/watchmal/DataTools/root_utils/notebooks/%s-log_train.csv\", prefix))\n",
    "    )\n",
    "    dfTest = rbind(\n",
    "      read.csv(sprintf(\"/home/lukasb/watchmal/DataTools/root_utils/notebooks/%s-log_test.csv\", prefix))\n",
    "    )\n",
    "\n",
    "\n",
    "    par(mfrow=c(1,2))\n",
    "    w=100\n",
    "    plot(c(ma(loss,w)) ~ c(ma(epoch,w)), dfTrain, 'l', col='green', xlab='Epoch', ylab='Loss', xaxs='i', xlim=c(0.,maxepochs))\n",
    "    lines(c(ma(loss,w/2)) ~ c(ma(epoch,w/2)), dfTest, col='blue')\n",
    "\n",
    "    plot(c(ma(accuracy,w)) ~ c(ma(epoch,w)), dfTrain, 'l', col='orange', xlab='Epoch', ylab='Accuracy', xlim=c(0.,maxepochs), ylim=c(0.,1.), xaxs='i', yaxs='i')\n",
    "    lines(c(ma(accuracy,w/2)) ~ c(ma(epoch,w/2)), dfTest, col='red')\n",
    "    \n",
    "    # or check if we are better than random prediction\n",
    "    # par(mfrow=c(1,1)); hist(dfTrain$accuracy, breaks=c(0.,0.5,1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_state(blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore_state(blob, 10128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train_log = pd.read_csv(blob.train_log.name)\n",
    "test_log  = pd.read_csv(blob.test_log.name)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12,8),facecolor='w')\n",
    "line11 = ax1.plot(train_log.epoch, train_log.loss, linewidth=2, label='Train loss', color='b', alpha=0.3)\n",
    "line12 = ax1.plot(test_log.epoch, test_log.loss, marker='o', markersize=12, linestyle='', label='Test loss', color='blue')\n",
    "ax1.set_xlabel('Epoch',fontweight='bold',fontsize=24,color='black')\n",
    "ax1.tick_params('x',colors='black',labelsize=18)\n",
    "ax1.set_ylabel('Loss', fontsize=24, fontweight='bold',color='b')\n",
    "ax1.tick_params('y',colors='b',labelsize=18)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "line21 = ax2.plot(train_log.epoch, train_log.accuracy, linewidth=2, label='Train accuracy', color='r', alpha=0.3)\n",
    "line22 = ax2.plot(test_log.epoch, test_log.accuracy, marker='o', markersize=12, linestyle='', label='Test accuracy', color='red')\n",
    "\n",
    "ax2.set_ylabel('Accuracy', fontsize=24, fontweight='bold',color='r')\n",
    "ax2.tick_params('y',colors='r',labelsize=18)\n",
    "ax2.set_ylim(0.,1.0)\n",
    "\n",
    "# added these four lines\n",
    "lines  = line11 + line12 + line21 + line22\n",
    "labels = [l.get_label() for l in lines]\n",
    "leg    = ax1.legend(lines, labels, fontsize=16, loc=5)\n",
    "leg_frame = leg.get_frame()\n",
    "leg_frame.set_facecolor('white')\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, n=3) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "epoch    = moving_average(np.array(train_log.epoch),40)\n",
    "accuracy = moving_average(np.array(train_log.accuracy),40)\n",
    "loss     = moving_average(np.array(train_log.loss),40)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12,8),facecolor='w')\n",
    "line11 = ax1.plot(train_log.epoch, train_log.loss, linewidth=2, label='Loss', color='b', alpha=0.3)\n",
    "line12 = ax1.plot(epoch, loss, label='Loss (averaged)', color='blue')\n",
    "ax1.set_xlabel('Epoch',fontweight='bold',fontsize=24,color='black')\n",
    "ax1.tick_params('x',colors='black',labelsize=18)\n",
    "ax1.set_ylabel('Loss', fontsize=24, fontweight='bold',color='b')\n",
    "ax1.tick_params('y',colors='b',labelsize=18)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "line21 = ax2.plot(train_log.epoch, train_log.accuracy, linewidth=2, label='Accuracy', color='r', alpha=0.3)\n",
    "line22 = ax2.plot(epoch, accuracy, label='Accuracy (averaged)', color='red')\n",
    "\n",
    "ax2.set_ylabel('Accuracy', fontsize=24, fontweight='bold',color='r')\n",
    "ax2.tick_params('y',colors='r',labelsize=18)\n",
    "ax2.set_ylim(0.,1.0)\n",
    "\n",
    "# added these four lines\n",
    "lines  = line11 + line12 + line21 + line22\n",
    "labels = [l.get_label() for l in lines]\n",
    "leg    = ax1.legend(lines, labels, fontsize=16, loc=5)\n",
    "leg_frame = leg.get_frame()\n",
    "leg_frame.set_facecolor('white')\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, n=3) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "train_epoch        = moving_average(np.array(train_log.epoch),40)\n",
    "train_accuracy_mav = moving_average(np.array(train_log.accuracy),40)\n",
    "train_loss_mav     = moving_average(np.array(train_log.loss),40)\n",
    "\n",
    "test_epoch        = moving_average(np.array(test_log.epoch),40)\n",
    "test_accuracy_mav = moving_average(np.array(test_log.accuracy),40)\n",
    "test_loss_mav     = moving_average(np.array(test_log.loss),40)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12,8),facecolor='w')\n",
    "line11 = ax1.plot(train_epoch, train_loss_mav, linewidth=2, label='Train loss (avg)', color='b', alpha=0.3)\n",
    "line12 = ax1.plot(test_epoch, test_loss_mav, label='Test loss (avg)', color='blue')\n",
    "ax1.set_xlabel('Epoch',fontweight='bold',fontsize=24,color='black')\n",
    "ax1.tick_params('x',colors='black',labelsize=18)\n",
    "ax1.set_ylabel('Loss', fontsize=24, fontweight='bold',color='b')\n",
    "ax1.tick_params('y',colors='b',labelsize=18)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "line21 = ax2.plot(train_epoch, train_accuracy_mav, linewidth=2, label='Train accuracy (avg)', color='r', alpha=0.3)\n",
    "line22 = ax2.plot(test_epoch, test_accuracy_mav, label='Test accuracy (avg)', color='red')\n",
    "\n",
    "ax2.set_ylabel('Accuracy', fontsize=24, fontweight='bold',color='r')\n",
    "ax2.tick_params('y',colors='r',labelsize=18)\n",
    "ax2.set_ylim(0.,1.0)\n",
    "\n",
    "# added these four lines\n",
    "lines  = line11 + line12 + line21 + line22\n",
    "labels = [l.get_label() for l in lines]\n",
    "leg    = ax1.legend(lines, labels, fontsize=16, loc=5)\n",
    "leg_frame = leg.get_frame()\n",
    "leg_frame.set_facecolor('white')\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "similar but maybe slightly worse performance than standard ResNet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(blob,data_loader):\n",
    "    label,prediction,accuracy=[],[],[]\n",
    "    with torch.no_grad():\n",
    "        # set the network to test (non-train) mode\n",
    "        blob.net.eval()\n",
    "        # create the result holder\n",
    "        index,label,prediction = [],[],[]\n",
    "        for i,data in enumerate(data_loader):\n",
    "            blob.data, blob.label = data[0:2]\n",
    "            res = forward(blob,train=False)\n",
    "            accuracy.append(res['accuracy'])\n",
    "            prediction.append(res['prediction'])\n",
    "            label.append(blob.label)\n",
    "            #if i==2: break\n",
    "        # report accuracy\n",
    "        del blob.data\n",
    "        del blob.label\n",
    "        del data\n",
    "\n",
    "        accuracy   = np.array(accuracy,dtype=np.float32)\n",
    "        label      = np.hstack(label)\n",
    "        prediction = np.hstack(prediction)\n",
    "    \n",
    "    return accuracy, label, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import plot_confusion_matrix\n",
    "\n",
    "inference_loader = DataLoader( test_ds,batch_size=64,shuffle=True,num_workers=2,collate_fn=HKCollate)\n",
    "\n",
    "accuracy,label,prediction = inference(blob,inference_loader)\n",
    "print('Accuracy mean',accuracy.mean(),'std',accuracy.std())\n",
    "plot_confusion_matrix(label,prediction,['e','mu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inferenceWithSoftmax(blob,data_loader):\n",
    "    label,prediction,accuracy,softmax=[],[],[],[]\n",
    "    # set the network to test (non-train) mode\n",
    "    blob.net.eval()\n",
    "    # create the result holder\n",
    "    index,label,prediction = [],[],[]\n",
    "    for i,data in enumerate(data_loader):\n",
    "        blob.data, blob.label = data[0:2]\n",
    "        res = forward(blob,train=False)\n",
    "        accuracy.append(res['accuracy'])\n",
    "        prediction.append(res['prediction'])\n",
    "        softmax.append(res['softmax'])\n",
    "        label.append(blob.label)\n",
    "        #if i==2: break\n",
    "    # report accuracy\n",
    "    accuracy   = np.array(accuracy,dtype=np.float32)\n",
    "    label      = np.hstack(label)\n",
    "    prediction = np.hstack(prediction)\n",
    "    softmax    = np.vstack(softmax)\n",
    "    \n",
    "    return accuracy, label, prediction, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy,label,prediction,softmax = inferenceWithSoftmax(blob,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-30, 30, 100)\n",
    "\n",
    "plt.hist(np.log((softmax[:,1]/softmax[:,0])[label == 0]), bins, alpha=0.5, label='true e')\n",
    "plt.hist(np.log((softmax[:,1]/softmax[:,0])[label == 1]), bins, alpha=0.5, label='true mu')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llr_order = np.argsort(np.log(softmax[:,1]/softmax[:,0]))\n",
    "cumsum_true0 = np.cumsum((label == 0)[llr_order]) / np.sum(label == 0)\n",
    "cumsum_true1 = np.cumsum((label == 1)[llr_order]) / np.sum(label == 1)\n",
    "\n",
    "plt.plot(cumsum_true0[cumsum_true1>0], 1./cumsum_true1[cumsum_true1>0])\n",
    "plt.xscale(\"linear\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"e signal efficiency\")\n",
    "plt.ylabel(\" background rejection\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    Nrows = 5\n",
    "    for r in range(Nrows):\n",
    "        Nplot = 6\n",
    "        fig, axes = plt.subplots(1, Nplot, figsize=(16,3),facecolor='w')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            blob.net.eval()\n",
    "            test_data = next(iter(test_loader))\n",
    "            blob.data,blob.label = test_data[0:2]\n",
    "            blob.data  = blob.data[0:Nplot,:,:,:]\n",
    "            blob.label = blob.label[0:Nplot]\n",
    "            res = forward(blob,True)\n",
    "\n",
    "            for ev in range(int(Nplot/2)):\n",
    "                for qt in range(2):\n",
    "                    #im = axes[2*ev+qt].imshow(blob.data[ev,:,:,qt])\n",
    "                    imshowRdBu(axes[2*ev+qt], torch.as_tensor(blob.data[ev,:,:,qt]))\n",
    "                    labelNames = ['e','']\n",
    "                    correct = (blob.label[ev] == res['prediction'][ev])\n",
    "                    if qt == 0:\n",
    "                        axes[2*ev+qt].set_title('%s pred: %s, true: %s' % ('' if correct else '', labelNames[blob.label[ev]], labelNames[res['prediction'][ev]]))\n",
    "                #cbar = fig.colorbar(im, ax=axes[0])\n",
    "            plt.show()\n",
    "            \n",
    "            del test_data\n",
    "            del blob.data\n",
    "            del blob.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inferenceWithSoftmax(blob,data_loader):\n",
    "    label,prediction,accuracy,softmax,positions,directions,energies=[],[],[],[],[],[],[]\n",
    "    # set the network to test (non-train) mode\n",
    "    blob.net.eval()\n",
    "    # create the result holder\n",
    "    index,label,prediction = [],[],[]\n",
    "    for i,data in enumerate(data_loader):\n",
    "        blob.data, blob.label = data[0:2]\n",
    "        position  = data[3]\n",
    "        direction = data[4]\n",
    "        energy    = data[5]\n",
    "        res = forward(blob,train=False)\n",
    "        accuracy.append(res['accuracy'])\n",
    "        prediction.append(res['prediction'])\n",
    "        softmax.append(res['softmax'])\n",
    "        label.append(blob.label)\n",
    "        positions.append(position)\n",
    "        directions.append(direction)\n",
    "        energies.append(energy)\n",
    "        #if i==2: break\n",
    "    # report accuracy\n",
    "    accuracy   = np.array(accuracy,dtype=np.float32)\n",
    "    label      = np.hstack(label)\n",
    "    prediction = np.hstack(prediction)\n",
    "    softmax    = np.vstack(softmax)\n",
    "    positions  = np.vstack(positions)\n",
    "    directions = np.vstack(directions)\n",
    "    energies   = np.concatenate(energies)\n",
    "    \n",
    "    return accuracy, label, prediction, softmax, positions, directions, energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy,label,prediction,softmax,positions,directions,energies = inferenceWithSoftmax(blob,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(energies[label==0], bins=50);\n",
    "plt.hist(energies[label==1], bins=50);\n",
    "plt.hist(energies[label==2], bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(positions[:,0]**2 + positions[:,2]**2, bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(370.**2 - (positions[:,0]**2 + positions[:,2]**2),bins=20);\n",
    "plt.xlabel('Distance to barrel wall [cm^2]');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(positions[:,1],bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(520. - np.abs(positions[:,1]),bins=50);\n",
    "plt.xlabel('Distance to top/bottom wall [cm]');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwall_caps = 520. - np.abs(positions[:,1]);\n",
    "dwall_barrel = 370. - np.sqrt(positions[:,0]**2 + positions[:,2]**2);\n",
    "dwall = np.amin(np.stack([dwall_caps,dwall_barrel]),0);\n",
    "\n",
    "plt.hist(dwall,bins=20);\n",
    "plt.xlabel('Distance to closest wall [cm]');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[10,6])\n",
    "plt.scatter(dwall[label==0], np.log((softmax[:,1]/softmax[:,0])[label == 0]), s=0.4, alpha=0.5, label='true e');\n",
    "plt.scatter(dwall[label==1], np.log((softmax[:,1]/softmax[:,0])[label == 1]), s=0.4, alpha=0.5, label='true mu');\n",
    "plt.legend(loc='upper right');\n",
    "plt.xlabel('Distance to closest wall [cm]');\n",
    "plt.ylabel('mu/e LLR');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-30, 30, 100)\n",
    "\n",
    "plt.hist(np.log((softmax[:,1]/softmax[:,0])[np.logical_and(label == 0, dwall > 50.)]), bins, alpha=0.5, label='true e')\n",
    "plt.hist(np.log((softmax[:,1]/softmax[:,0])[np.logical_and(label == 1, dwall > 50.)]), bins, alpha=0.5, label='true mu')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('dwall > 50 cm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-30, 30, 100)\n",
    "\n",
    "plt.hist(np.log((softmax[:,1]/softmax[:,0])[np.logical_and(label == 0, dwall > 100.)]), bins, alpha=0.5, label='true e')\n",
    "plt.hist(np.log((softmax[:,1]/softmax[:,0])[np.logical_and(label == 1, dwall > 100.)]), bins, alpha=0.5, label='true mu')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('dwall > 100 cm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    Nrows = 5\n",
    "    for r in range(Nrows):\n",
    "        Nplot = 6\n",
    "        fig, axes = plt.subplots(1, Nplot, figsize=(16,3),facecolor='w')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            plotted = 0\n",
    "            for test_data in iter(test_loader):\n",
    "                if plotted >= Nplot:\n",
    "                    break\n",
    "                blob.net.eval()\n",
    "                blob.data,blob.label = test_data[0:2]\n",
    "                #blob.data  = blob.data[0:Nplot,:,:,:]\n",
    "                #blob.label = blob.label[0:Nplot]\n",
    "                res = forward(blob,train=False)\n",
    "\n",
    "                llr = np.log(res['softmax'][:,1]/res['softmax'][:,0])\n",
    "                position  = test_data[3]\n",
    "                direction = test_data[4]\n",
    "                dwall_caps = 520. - np.abs(positions[:,1]);\n",
    "                dwall_barrel = 370. - np.sqrt(positions[:,0]**2 + positions[:,2]**2);\n",
    "                dwall = np.amin(np.stack([dwall_caps,dwall_barrel]),0);\n",
    "\n",
    "                for ev in range(len(blob.label)):\n",
    "                    if dwall[ev] < 50. or np.abs(llr[ev]) > 2:\n",
    "                        continue\n",
    "                    for qt in range(2):\n",
    "                        #im = axes[2*ev+qt].imshow(blob.data[ev,:,:,qt])\n",
    "                        imshowRdBu(axes[plotted], torch.as_tensor(blob.data[ev,:,:,qt]))\n",
    "                        labelNames = ['e','']\n",
    "                        correct = (blob.label[ev] == res['prediction'][ev])\n",
    "                        if qt == 0:\n",
    "                            axes[plotted].set_title('%s pred: %s, true: %s' % ('' if correct else '', labelNames[blob.label[ev]], labelNames[res['prediction'][ev]]))\n",
    "                        plotted += 1\n",
    "                    if plotted >= Nplot:\n",
    "                        break\n",
    "            plt.show()\n",
    "            \n",
    "            del test_data\n",
    "            del blob.data\n",
    "            del blob.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[10,6])\n",
    "plt.scatter(energies[label==0]-np.sqrt(0.511**2+0.57**2), np.log((softmax[:,1]/softmax[:,0])[label == 0]), s=0.4, alpha=0.5, label='true e');\n",
    "plt.scatter(energies[label==1]-np.sqrt(105.7**2+118**2), np.log((softmax[:,1]/softmax[:,0])[label == 1]), s=0.4, alpha=0.5, label='true mu');\n",
    "plt.legend(loc='upper right');\n",
    "plt.xlabel('Energy above Cherenkov threshold [MeV]');\n",
    "plt.ylabel('mu/e LLR');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[10,6])\n",
    "plt.scatter(energies[np.logical_and(label==0, dwall> 50.)]-np.sqrt(0.511**2+0.57**2), np.log((softmax[:,1]/softmax[:,0])[np.logical_and(label == 0, dwall > 50.)]), s=0.4, alpha=0.5, label='true e');\n",
    "plt.scatter(energies[np.logical_and(label==1, dwall> 50.)]-np.sqrt(105.7**2+118**2 ), np.log((softmax[:,1]/softmax[:,0])[np.logical_and(label == 1, dwall > 50.)]), s=0.4, alpha=0.5, label='true mu');\n",
    "plt.legend(loc='upper right');\n",
    "plt.xlabel('Energy above Cherenkov threshold [MeV]');\n",
    "plt.ylabel('mu/e LLR');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[10,6])\n",
    "plt.scatter(energies[np.logical_and(label==0, dwall> 100.)]-np.sqrt(0.511**2+0.57**2), np.log((softmax[:,1]/softmax[:,0])[np.logical_and(label == 0, dwall > 100.)]), s=0.4, alpha=0.5, label='true e');\n",
    "plt.scatter(energies[np.logical_and(label==1, dwall> 100.)]-np.sqrt(105.7**2+118**2 ), np.log((softmax[:,1]/softmax[:,0])[np.logical_and(label == 1, dwall > 100.)]), s=0.4, alpha=0.5, label='true mu');\n",
    "plt.legend(loc='upper right');\n",
    "plt.xlabel('Energy above Cherenkov threshold [MeV]');\n",
    "plt.ylabel('mu/e LLR');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    Nrows = 5\n",
    "    for r in range(Nrows):\n",
    "        Nplot = 6\n",
    "        fig, axes = plt.subplots(1, Nplot, figsize=(16,3),facecolor='w')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            plotted = 0\n",
    "            for test_data in iter(test_loader):\n",
    "                if plotted >= Nplot:\n",
    "                    break\n",
    "                blob.net.eval()\n",
    "                blob.data,blob.label = test_data[0:2]\n",
    "                #blob.data  = blob.data[0:Nplot,:,:,:]\n",
    "                #blob.label = blob.label[0:Nplot]\n",
    "                res = forward(blob,train=False)\n",
    "\n",
    "                llr = np.log(res['softmax'][:,1]/res['softmax'][:,0])\n",
    "                position  = test_data[3]\n",
    "                direction = test_data[4]\n",
    "                energy    = test_data[5]\n",
    "                dwall_caps = 520. - np.abs(positions[:,1]);\n",
    "                dwall_barrel = 370. - np.sqrt(positions[:,0]**2 + positions[:,2]**2);\n",
    "                dwall = np.amin(np.stack([dwall_caps,dwall_barrel]),0);\n",
    "\n",
    "                for ev in range(len(blob.label)):\n",
    "                    Ekin = energy[ev] - np.where(blob.label[ev] == 1, np.sqrt(105.7**2+118**2 ), np.sqrt(0.511**2+0.57**2))\n",
    "                    if dwall[ev] < 100. or np.abs(llr[ev]+4.) > 2 or Ekin < 200.:\n",
    "                        continue\n",
    "                    for qt in range(2):\n",
    "                        #im = axes[2*ev+qt].imshow(blob.data[ev,:,:,qt])\n",
    "                        imshowRdBu(axes[plotted], torch.as_tensor(blob.data[ev,:,:,qt]))\n",
    "                        labelNames = ['e','']\n",
    "                        correct = (blob.label[ev] == res['prediction'][ev])\n",
    "                        if qt == 0:\n",
    "                            axes[plotted].set_title('%s pred: %s, true: %s' % ('' if correct else '', labelNames[blob.label[ev]], labelNames[res['prediction'][ev]]))\n",
    "                        plotted += 1\n",
    "                    if plotted >= Nplot:\n",
    "                        break\n",
    "            plt.show()\n",
    "            \n",
    "            del test_data\n",
    "            del blob.data\n",
    "            del blob.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label,prediction,softmax,positions,directions\n",
    "np.savetxt('%s-inference.csv' % blob.prefix,\n",
    "           np.hstack([np.expand_dims(label,1),np.expand_dims(prediction,1),softmax,positions,directions]),\n",
    "           delimiter=','\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with resnet without time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label,prediction,softmax,positions,directions\n",
    "infWithTime = np.loadtxt('%s-inference.csv' % blob.prefix, delimiter=',')\n",
    "infChrgOnly = np.loadtxt('%s-inference.csv' % '20200708-03-IWCD-ResNetGeom-01-e-mu', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sepCurve(fig, infSet, lineLabel):\n",
    "    label   = infSet[:,0].astype(int)\n",
    "    softmax = infSet[:,2:4]\n",
    "    \n",
    "    llr_order = np.argsort(np.log(softmax[:,1]/softmax[:,0]))\n",
    "    cumsum_true0 = np.cumsum((label == 0)[llr_order]) / np.sum(label == 0)\n",
    "    cumsum_true1 = np.cumsum((label == 1)[llr_order]) / np.sum(label == 1)\n",
    "    \n",
    "    fig.plot(cumsum_true0[cumsum_true1>0], 1./cumsum_true1[cumsum_true1>0], label=lineLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[10,6])\n",
    "sepCurve(plt, infChrgOnly, 'Charge')\n",
    "sepCurve(plt, infWithTime, 'Charge+Time')\n",
    "plt.xscale(\"linear\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"e signal efficiency\")\n",
    "plt.ylabel(\" background rejection\")\n",
    "plt.grid()\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train for 4 more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from utils.utils import progress_bar, CSVData\n",
    "\n",
    "blob.train_log, blob.test_log = CSVData('%s-log_train_epoch4to8.csv' % blob.prefix), CSVData('%s-log_test_epoch4to8.csv' % blob.prefix)\n",
    "\n",
    "# Define train period. \"epoch\" = N image consumption where N is the total number of train samples.\n",
    "TRAIN_EPOCH=8.0\n",
    "# Set the network to training mode\n",
    "blob.net.train()\n",
    "\n",
    "# Start training\n",
    "while int(blob.epoch+0.5) < TRAIN_EPOCH:\n",
    "    print('Epoch',int(blob.epoch+0.5),'Starting @',time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "    # Create a progress bar for this epoch\n",
    "    progress = display(progress_bar(0,len(train_loader)),display_id=True)\n",
    "    # Loop over data samples and into the network forward function\n",
    "    for i,data in enumerate(train_loader):\n",
    "        # Data and label\n",
    "        blob.data,blob.label = data[0:2]\n",
    "        # Call forward: make a prediction & measure the average error\n",
    "        res = forward(blob,True)\n",
    "        # Call backward: backpropagate error and update weights\n",
    "        backward(blob)\n",
    "        # Epoch update\n",
    "        blob.epoch += 1./len(train_loader)\n",
    "        blob.iteration += 1\n",
    "        \n",
    "        #\n",
    "        # Log/Report\n",
    "        #\n",
    "        # Record the current performance on train set\n",
    "        blob.train_log.record(['iteration','epoch','accuracy','loss'],[blob.iteration,blob.epoch,res['accuracy'],res['loss']])\n",
    "        blob.train_log.write()\n",
    "        # once in a while, report\n",
    "        if i==0 or (i+1)%10 == 0:\n",
    "            message = '... Iteration %d ... Epoch %1.2f ... Loss %1.3f ... Accuracy %1.3f' % (blob.iteration,blob.epoch,res['loss'],res['accuracy'])\n",
    "            progress.update(progress_bar((i+1),len(train_loader),message))\n",
    "        # more rarely, run validation\n",
    "        if (i+1)%40 == 0:\n",
    "            with torch.no_grad():\n",
    "                blob.net.eval()\n",
    "                test_data = next(iter(test_loader))\n",
    "                blob.data,blob.label = test_data[0:2]\n",
    "                res = forward(blob,False)\n",
    "                blob.test_log.record(['iteration','epoch','accuracy','loss'],[blob.iteration,blob.epoch,res['accuracy'],res['loss']])\n",
    "                blob.test_log.write()\n",
    "            blob.net.train()\n",
    "        # even more rarely, save state\n",
    "        if (i+1)%400 == 0:\n",
    "            with torch.no_grad():\n",
    "                blob.net.eval()\n",
    "                save_state(blob)\n",
    "            blob.net.train()\n",
    "        if blob.epoch >= TRAIN_EPOCH:\n",
    "            break\n",
    "    message = '... Iteration %d ... Epoch %1.2f ... Loss %1.3f ... Accuracy %1.3f' % (blob.iteration,blob.epoch,res['loss'],res['accuracy'])\n",
    "    #print(message)\n",
    "    progress.update(progress_bar((i+1),len(train_loader),message))\n",
    "\n",
    "blob.test_log.close()\n",
    "blob.train_log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_state(blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train_log = pd.read_csv(blob.train_log.name)\n",
    "test_log  = pd.read_csv(blob.test_log.name)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12,8),facecolor='w')\n",
    "line11 = ax1.plot(train_log.epoch, train_log.loss, linewidth=2, label='Train loss', color='b', alpha=0.3)\n",
    "line12 = ax1.plot(test_log.epoch, test_log.loss, marker='o', markersize=12, linestyle='', label='Test loss', color='blue')\n",
    "ax1.set_xlabel('Epoch',fontweight='bold',fontsize=24,color='black')\n",
    "ax1.tick_params('x',colors='black',labelsize=18)\n",
    "ax1.set_ylabel('Loss', fontsize=24, fontweight='bold',color='b')\n",
    "ax1.tick_params('y',colors='b',labelsize=18)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "line21 = ax2.plot(train_log.epoch, train_log.accuracy, linewidth=2, label='Train accuracy', color='r', alpha=0.3)\n",
    "line22 = ax2.plot(test_log.epoch, test_log.accuracy, marker='o', markersize=12, linestyle='', label='Test accuracy', color='red')\n",
    "\n",
    "ax2.set_ylabel('Accuracy', fontsize=24, fontweight='bold',color='r')\n",
    "ax2.tick_params('y',colors='r',labelsize=18)\n",
    "ax2.set_ylim(0.,1.0)\n",
    "\n",
    "# added these four lines\n",
    "lines  = line11 + line12 + line21 + line22\n",
    "labels = [l.get_label() for l in lines]\n",
    "leg    = ax1.legend(lines, labels, fontsize=16, loc=5)\n",
    "leg_frame = leg.get_frame()\n",
    "leg_frame.set_facecolor('white')\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import plot_confusion_matrix\n",
    "\n",
    "inference_loader = DataLoader( test_ds,batch_size=64,shuffle=True,num_workers=2,collate_fn=HKCollate)\n",
    "\n",
    "accuracy,label,prediction = inference(blob,inference_loader)\n",
    "print('Accuracy mean',accuracy.mean(),'std',accuracy.std())\n",
    "plot_confusion_matrix(label,prediction,['e','mu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy,label,prediction,softmax,positions,directions,energies = inferenceWithSoftmax(blob,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-30, 30, 100)\n",
    "\n",
    "plt.hist(np.log((softmax[:,1]/softmax[:,0])[label == 0]), bins, alpha=0.5, label='true e')\n",
    "plt.hist(np.log((softmax[:,1]/softmax[:,0])[label == 1]), bins, alpha=0.5, label='true mu')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llr_order = np.argsort(np.log(softmax[:,1]/softmax[:,0]))\n",
    "cumsum_true0 = np.cumsum((label == 0)[llr_order]) / np.sum(label == 0)\n",
    "cumsum_true1 = np.cumsum((label == 1)[llr_order]) / np.sum(label == 1)\n",
    "\n",
    "plt.plot(cumsum_true0[cumsum_true1>0], 1./cumsum_true1[cumsum_true1>0])\n",
    "plt.xscale(\"linear\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"e signal efficiency\")\n",
    "plt.ylabel(\" background rejection\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcdir = '/home/lukasb/watchmal/data/IWCDmPMT_4pi_full_tank/h5_topo'\n",
    "pnames = ('e-','mu-')\n",
    "files = ['%s/%s/IWCDmPMT_4pi_full_tank_%s_E0to1000MeV_unif-pos-R371-y521cm_4pi-dir_3000evts_%d.h5' % (mcdir,pname,pname,bch+100) for bch in range(300) for pname in pnames]\n",
    "\n",
    "train_ds = H5Dataset(files,start_fraction=0.0,use_fraction=0.9);\n",
    "test_ds  = H5Dataset(files,start_fraction=0.9,use_fraction=0.1);\n",
    "\n",
    "# for training\n",
    "train_loader=DataLoader(train_ds,batch_size=32,shuffle=True,num_workers=4,collate_fn=HKCollate)\n",
    "# for validation\n",
    "test_loader =DataLoader( test_ds,batch_size=64,shuffle=True,num_workers=2,collate_fn=HKCollate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from utils.utils import progress_bar, CSVData\n",
    "\n",
    "blob.train_log, blob.test_log = CSVData('%s-log_train_epoch8to12.csv' % blob.prefix), CSVData('%s-log_test_epoch8to12.csv' % blob.prefix)\n",
    "\n",
    "# Define train period. \"epoch\" = N image consumption where N is the total number of train samples.\n",
    "TRAIN_EPOCH=12.0\n",
    "# Set the network to training mode\n",
    "blob.net.train()\n",
    "\n",
    "# Start training\n",
    "while int(blob.epoch+0.5) < TRAIN_EPOCH:\n",
    "    print('Epoch',int(blob.epoch+0.5),'Starting @',time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "    # Create a progress bar for this epoch\n",
    "    progress = display(progress_bar(0,len(train_loader)),display_id=True)\n",
    "    # Loop over data samples and into the network forward function\n",
    "    for i,data in enumerate(train_loader):\n",
    "        # Data and label\n",
    "        blob.data,blob.label = data[0:2]\n",
    "        # Call forward: make a prediction & measure the average error\n",
    "        res = forward(blob,True)\n",
    "        # Call backward: backpropagate error and update weights\n",
    "        backward(blob)\n",
    "        # Epoch update\n",
    "        blob.epoch += 1./len(train_loader)\n",
    "        blob.iteration += 1\n",
    "        \n",
    "        #\n",
    "        # Log/Report\n",
    "        #\n",
    "        # Record the current performance on train set\n",
    "        blob.train_log.record(['iteration','epoch','accuracy','loss'],[blob.iteration,blob.epoch,res['accuracy'],res['loss']])\n",
    "        blob.train_log.write()\n",
    "        # once in a while, report\n",
    "        if i==0 or (i+1)%10 == 0:\n",
    "            message = '... Iteration %d ... Epoch %1.2f ... Loss %1.3f ... Accuracy %1.3f' % (blob.iteration,blob.epoch,res['loss'],res['accuracy'])\n",
    "            progress.update(progress_bar((i+1),len(train_loader),message))\n",
    "        # more rarely, run validation\n",
    "        if (i+1)%40 == 0:\n",
    "            with torch.no_grad():\n",
    "                blob.net.eval()\n",
    "                test_data = next(iter(test_loader))\n",
    "                blob.data,blob.label = test_data[0:2]\n",
    "                res = forward(blob,False)\n",
    "                blob.test_log.record(['iteration','epoch','accuracy','loss'],[blob.iteration,blob.epoch,res['accuracy'],res['loss']])\n",
    "                blob.test_log.write()\n",
    "            blob.net.train()\n",
    "        # even more rarely, save state\n",
    "        if (i+1)%400 == 0:\n",
    "            with torch.no_grad():\n",
    "                blob.net.eval()\n",
    "                save_state(blob)\n",
    "            blob.net.train()\n",
    "        if blob.epoch >= TRAIN_EPOCH:\n",
    "            break\n",
    "    message = '... Iteration %d ... Epoch %1.2f ... Loss %1.3f ... Accuracy %1.3f' % (blob.iteration,blob.epoch,res['loss'],res['accuracy'])\n",
    "    #print(message)\n",
    "    progress.update(progress_bar((i+1),len(train_loader),message))\n",
    "\n",
    "blob.test_log.close()\n",
    "blob.train_log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_state(blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train_log = pd.read_csv(blob.train_log.name)\n",
    "test_log  = pd.read_csv(blob.test_log.name)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12,8),facecolor='w')\n",
    "line11 = ax1.plot(train_log.epoch, train_log.loss, linewidth=2, label='Train loss', color='b', alpha=0.3)\n",
    "line12 = ax1.plot(test_log.epoch, test_log.loss, marker='o', markersize=12, linestyle='', label='Test loss', color='blue')\n",
    "ax1.set_xlabel('Epoch',fontweight='bold',fontsize=24,color='black')\n",
    "ax1.tick_params('x',colors='black',labelsize=18)\n",
    "ax1.set_ylabel('Loss', fontsize=24, fontweight='bold',color='b')\n",
    "ax1.tick_params('y',colors='b',labelsize=18)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "line21 = ax2.plot(train_log.epoch, train_log.accuracy, linewidth=2, label='Train accuracy', color='r', alpha=0.3)\n",
    "line22 = ax2.plot(test_log.epoch, test_log.accuracy, marker='o', markersize=12, linestyle='', label='Test accuracy', color='red')\n",
    "\n",
    "ax2.set_ylabel('Accuracy', fontsize=24, fontweight='bold',color='r')\n",
    "ax2.tick_params('y',colors='r',labelsize=18)\n",
    "ax2.set_ylim(0.,1.0)\n",
    "\n",
    "# added these four lines\n",
    "lines  = line11 + line12 + line21 + line22\n",
    "labels = [l.get_label() for l in lines]\n",
    "leg    = ax1.legend(lines, labels, fontsize=16, loc=5)\n",
    "leg_frame = leg.get_frame()\n",
    "leg_frame.set_facecolor('white')\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy,label,prediction,softmax,positions,directions,energies = inferenceWithSoftmax(blob,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-30, 30, 100)\n",
    "\n",
    "plt.hist(np.log((softmax[:,1]/softmax[:,0])[label == 0]), bins, alpha=0.5, label='true e')\n",
    "plt.hist(np.log((softmax[:,1]/softmax[:,0])[label == 1]), bins, alpha=0.5, label='true mu')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llr_order = np.argsort(np.log(softmax[:,1]/softmax[:,0]))\n",
    "cumsum_true0 = np.cumsum((label == 0)[llr_order]) / np.sum(label == 0)\n",
    "cumsum_true1 = np.cumsum((label == 1)[llr_order]) / np.sum(label == 1)\n",
    "\n",
    "plt.plot(cumsum_true0[cumsum_true1>0], 1./cumsum_true1[cumsum_true1>0])\n",
    "plt.xscale(\"linear\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"e signal efficiency\")\n",
    "plt.ylabel(\" background rejection\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label,prediction,softmax,positions,directions\n",
    "np.savetxt('%s-inference_epoch12.csv' % blob.prefix,\n",
    "           np.hstack([np.expand_dims(label,1),np.expand_dims(prediction,1),softmax,positions,directions]),\n",
    "           delimiter=','\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train until epoch 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from utils.utils import progress_bar, CSVData\n",
    "\n",
    "blob.train_log, blob.test_log = CSVData('%s-log_train_epoch12to16.csv' % blob.prefix), CSVData('%s-log_test_epoch12to16.csv' % blob.prefix)\n",
    "\n",
    "# Define train period. \"epoch\" = N image consumption where N is the total number of train samples.\n",
    "TRAIN_EPOCH=16.0\n",
    "# Set the network to training mode\n",
    "blob.net.train()\n",
    "\n",
    "# Start training\n",
    "while int(blob.epoch+0.5) < TRAIN_EPOCH:\n",
    "    print('Epoch',int(blob.epoch+0.5),'Starting @',time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "    # Create a progress bar for this epoch\n",
    "    progress = display(progress_bar(0,len(train_loader)),display_id=True)\n",
    "    # Loop over data samples and into the network forward function\n",
    "    for i,data in enumerate(train_loader):\n",
    "        # Data and label\n",
    "        blob.data,blob.label = data[0:2]\n",
    "        # Call forward: make a prediction & measure the average error\n",
    "        res = forward(blob,True)\n",
    "        # Call backward: backpropagate error and update weights\n",
    "        backward(blob)\n",
    "        # Epoch update\n",
    "        blob.epoch += 1./len(train_loader)\n",
    "        blob.iteration += 1\n",
    "        \n",
    "        #\n",
    "        # Log/Report\n",
    "        #\n",
    "        # Record the current performance on train set\n",
    "        blob.train_log.record(['iteration','epoch','accuracy','loss'],[blob.iteration,blob.epoch,res['accuracy'],res['loss']])\n",
    "        blob.train_log.write()\n",
    "        # once in a while, report\n",
    "        if i==0 or (i+1)%10 == 0:\n",
    "            message = '... Iteration %d ... Epoch %1.2f ... Loss %1.3f ... Accuracy %1.3f' % (blob.iteration,blob.epoch,res['loss'],res['accuracy'])\n",
    "            progress.update(progress_bar((i+1),len(train_loader),message))\n",
    "        # more rarely, run validation\n",
    "        if (i+1)%40 == 0:\n",
    "            with torch.no_grad():\n",
    "                blob.net.eval()\n",
    "                test_data = next(iter(test_loader))\n",
    "                blob.data,blob.label = test_data[0:2]\n",
    "                res = forward(blob,False)\n",
    "                blob.test_log.record(['iteration','epoch','accuracy','loss'],[blob.iteration,blob.epoch,res['accuracy'],res['loss']])\n",
    "                blob.test_log.write()\n",
    "            blob.net.train()\n",
    "        # even more rarely, save state\n",
    "        if (i+1)%400 == 0:\n",
    "            with torch.no_grad():\n",
    "                blob.net.eval()\n",
    "                save_state(blob)\n",
    "            blob.net.train()\n",
    "        if blob.epoch >= TRAIN_EPOCH:\n",
    "            break\n",
    "    message = '... Iteration %d ... Epoch %1.2f ... Loss %1.3f ... Accuracy %1.3f' % (blob.iteration,blob.epoch,res['loss'],res['accuracy'])\n",
    "    #print(message)\n",
    "    progress.update(progress_bar((i+1),len(train_loader),message))\n",
    "\n",
    "blob.test_log.close()\n",
    "blob.train_log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_state(blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train_log = pd.read_csv(blob.train_log.name)\n",
    "test_log  = pd.read_csv(blob.test_log.name)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12,8),facecolor='w')\n",
    "line11 = ax1.plot(train_log.epoch, train_log.loss, linewidth=2, label='Train loss', color='b', alpha=0.3)\n",
    "line12 = ax1.plot(test_log.epoch, test_log.loss, marker='o', markersize=12, linestyle='', label='Test loss', color='blue')\n",
    "ax1.set_xlabel('Epoch',fontweight='bold',fontsize=24,color='black')\n",
    "ax1.tick_params('x',colors='black',labelsize=18)\n",
    "ax1.set_ylabel('Loss', fontsize=24, fontweight='bold',color='b')\n",
    "ax1.tick_params('y',colors='b',labelsize=18)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "line21 = ax2.plot(train_log.epoch, train_log.accuracy, linewidth=2, label='Train accuracy', color='r', alpha=0.3)\n",
    "line22 = ax2.plot(test_log.epoch, test_log.accuracy, marker='o', markersize=12, linestyle='', label='Test accuracy', color='red')\n",
    "\n",
    "ax2.set_ylabel('Accuracy', fontsize=24, fontweight='bold',color='r')\n",
    "ax2.tick_params('y',colors='r',labelsize=18)\n",
    "ax2.set_ylim(0.,1.0)\n",
    "\n",
    "# added these four lines\n",
    "lines  = line11 + line12 + line21 + line22\n",
    "labels = [l.get_label() for l in lines]\n",
    "leg    = ax1.legend(lines, labels, fontsize=16, loc=5)\n",
    "leg_frame = leg.get_frame()\n",
    "leg_frame.set_facecolor('white')\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy,label,prediction,softmax,positions,directions,energies = inferenceWithSoftmax(blob,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-30, 30, 100)\n",
    "\n",
    "plt.hist(np.log((softmax[:,1]/softmax[:,0])[label == 0]), bins, alpha=0.5, label='true e')\n",
    "plt.hist(np.log((softmax[:,1]/softmax[:,0])[label == 1]), bins, alpha=0.5, label='true mu')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llr_order = np.argsort(np.log(softmax[:,1]/softmax[:,0]))\n",
    "cumsum_true0 = np.cumsum((label == 0)[llr_order]) / np.sum(label == 0)\n",
    "cumsum_true1 = np.cumsum((label == 1)[llr_order]) / np.sum(label == 1)\n",
    "\n",
    "plt.plot(cumsum_true0[cumsum_true1>0], 1./cumsum_true1[cumsum_true1>0])\n",
    "plt.xscale(\"linear\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"e signal efficiency\")\n",
    "plt.ylabel(\" background rejection\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label,prediction,softmax,positions,directions\n",
    "np.savetxt('%s-inference_epoch16.csv' % blob.prefix,\n",
    "           np.hstack([np.expand_dims(label,1),np.expand_dims(prediction,1),softmax,positions,directions]),\n",
    "           delimiter=','\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label,prediction,softmax,positions,directions\n",
    "infEpoch4 = np.loadtxt('%s-inference.csv' % blob.prefix, delimiter=',')\n",
    "#infEpoch8 = np.loadtxt('%s-inference_epoch8.csv' % blob.prefix, delimiter=',')\n",
    "infEpoch12 = np.loadtxt('%s-inference_epoch12.csv' % blob.prefix, delimiter=',')\n",
    "infEpoch16 = np.loadtxt('%s-inference_epoch16.csv' % blob.prefix, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[10,6])\n",
    "sepCurve(plt, infEpoch4, '4 epochs')\n",
    "sepCurve(plt, infEpoch12, '12 epochs')\n",
    "sepCurve(plt, infEpoch16, '16 epochs')\n",
    "plt.xscale(\"linear\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"e signal efficiency\")\n",
    "plt.ylabel(\" background rejection\")\n",
    "plt.grid()\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
